package daemon

import (
	"context"
	"encoding/json"
	"errors"
	"fmt"
	"net"
	"net/http"
	"os"
	"path/filepath"
	"strings"
	"sync"
	"syscall"
	"testing"
	"time"

	"github.com/victorarias/attn/internal/client"
	"github.com/victorarias/attn/internal/config"
	"github.com/victorarias/attn/internal/github"
	"github.com/victorarias/attn/internal/github/mockserver"
	"github.com/victorarias/attn/internal/protocol"
	"github.com/victorarias/attn/internal/ptybackend"
	"github.com/victorarias/attn/internal/store"
	"nhooyr.io/websocket"
)

type countingClassifier struct {
	state string
	mu    sync.Mutex
	calls int
}

func (c *countingClassifier) Classify(text string, timeout time.Duration) (string, error) {
	c.mu.Lock()
	c.calls++
	c.mu.Unlock()
	return c.state, nil
}

func (c *countingClassifier) CallCount() int {
	c.mu.Lock()
	defer c.mu.Unlock()
	return c.calls
}

type blockingClassifier struct {
	state   string
	started chan struct{}
	release chan struct{}
	mu      sync.Mutex
	calls   int
}

type errorClassifier struct {
	state string
	err   error
}

func (c *errorClassifier) Classify(text string, timeout time.Duration) (string, error) {
	return c.state, c.err
}

func newBlockingClassifier(state string) *blockingClassifier {
	return &blockingClassifier{
		state:   state,
		started: make(chan struct{}, 1),
		release: make(chan struct{}),
	}
}

func (c *blockingClassifier) Classify(text string, timeout time.Duration) (string, error) {
	c.mu.Lock()
	c.calls++
	c.mu.Unlock()
	select {
	case c.started <- struct{}{}:
	default:
	}
	<-c.release
	return c.state, nil
}

func (c *blockingClassifier) CallCount() int {
	c.mu.Lock()
	defer c.mu.Unlock()
	return c.calls
}

func TestMain(m *testing.M) {
	// Keep daemon package tests on embedded PTY by default to avoid spawning
	// large numbers of worker subprocesses. Tests that need worker behavior
	// explicitly override these with t.Setenv.
	_ = os.Setenv("ATTN_PTY_BACKEND", "embedded")
	_ = os.Setenv("ATTN_PTY_SKIP_STARTUP_PROBE", "1")
	os.Exit(m.Run())
}

// waitForSocket waits for a unix socket to be ready for connections.
// This is more reliable than fixed sleeps, especially in CI environments.
func waitForSocket(t *testing.T, sockPath string, timeout time.Duration) {
	t.Helper()
	deadline := time.Now().Add(timeout)
	for time.Now().Before(deadline) {
		conn, err := net.DialTimeout("unix", sockPath, 10*time.Millisecond)
		if err == nil {
			conn.Close()
			return
		}
		time.Sleep(10 * time.Millisecond)
	}
	t.Fatalf("socket %s not ready after %v", sockPath, timeout)
}

func shortTempDir(t *testing.T) string {
	t.Helper()
	// Unix socket paths are length-limited (notably on macOS). The default
	// `t.TempDir()` path can be too long, so keep the base dir short.
	base := "/tmp"
	if _, err := os.Stat(base); err != nil {
		base = ""
	}
	dir, err := os.MkdirTemp(base, "attn-")
	if err != nil {
		t.Fatalf("MkdirTemp() error: %v", err)
	}
	t.Cleanup(func() { _ = os.RemoveAll(dir) })
	return dir
}

func TestDaemon_RegisterAndQuery(t *testing.T) {
	t.Setenv("ATTN_WS_PORT", "19900")

	tmpDir := t.TempDir()
	sockPath := filepath.Join(tmpDir, "test.sock")

	d := NewForTesting(sockPath)
	go d.Start()
	defer d.Stop()

	waitForSocket(t, sockPath, 5*time.Second)

	c := client.New(sockPath)

	// Register a session
	err := c.Register("sess-1", "drumstick", "/home/user/project")
	if err != nil {
		t.Fatalf("Register error: %v", err)
	}

	// Query all sessions
	sessions, err := c.Query("")
	if err != nil {
		t.Fatalf("Query error: %v", err)
	}
	if len(sessions) != 1 {
		t.Fatalf("got %d sessions, want 1", len(sessions))
	}
	if sessions[0].Label != "drumstick" {
		t.Errorf("Label = %q, want %q", sessions[0].Label, "drumstick")
	}
}

func TestDaemon_StateUpdate(t *testing.T) {
	t.Setenv("ATTN_WS_PORT", "19901")

	tmpDir := t.TempDir()
	sockPath := filepath.Join(tmpDir, "test.sock")

	d := NewForTesting(sockPath)
	go d.Start()
	defer d.Stop()

	waitForSocket(t, sockPath, 5*time.Second)

	c := client.New(sockPath)

	// Register
	c.Register("sess-1", "test", "/tmp")

	// Update state
	err := c.UpdateState("sess-1", protocol.StateWaitingInput)
	if err != nil {
		t.Fatalf("UpdateState error: %v", err)
	}

	// Query waiting
	sessions, err := c.Query(protocol.StateWaitingInput)
	if err != nil {
		t.Fatalf("Query error: %v", err)
	}
	if len(sessions) != 1 {
		t.Fatalf("got %d waiting sessions, want 1", len(sessions))
	}
}

func TestDaemon_Unregister(t *testing.T) {
	t.Setenv("ATTN_WS_PORT", "19902")

	tmpDir := t.TempDir()
	sockPath := filepath.Join(tmpDir, "test.sock")

	d := NewForTesting(sockPath)
	go d.Start()
	defer d.Stop()

	waitForSocket(t, sockPath, 5*time.Second)

	c := client.New(sockPath)

	c.Register("sess-1", "test", "/tmp")
	c.Unregister("sess-1")

	sessions, _ := c.Query("")
	if len(sessions) != 0 {
		t.Errorf("got %d sessions after unregister, want 0", len(sessions))
	}
}

func TestDaemon_MultipleSessions(t *testing.T) {
	t.Setenv("ATTN_WS_PORT", "19903")

	tmpDir := t.TempDir()
	sockPath := filepath.Join(tmpDir, "test.sock")

	d := NewForTesting(sockPath)
	go d.Start()
	defer d.Stop()

	waitForSocket(t, sockPath, 5*time.Second)

	c := client.New(sockPath)

	// Register multiple sessions (all start as launching)
	c.Register("1", "one", "/tmp/1")
	c.Register("2", "two", "/tmp/2")
	c.Register("3", "three", "/tmp/3")

	// Update one to working
	c.UpdateState("2", protocol.StateWorking)

	// Query launching (sessions 1 and 3)
	launching, _ := c.Query(protocol.StateLaunching)
	if len(launching) != 2 {
		t.Errorf("got %d launching, want 2", len(launching))
	}

	// Query working (session 2)
	working, _ := c.Query(protocol.StateWorking)
	if len(working) != 1 {
		t.Errorf("got %d working, want 1", len(working))
	}
}

func TestDaemon_SocketCleanup(t *testing.T) {
	t.Setenv("ATTN_WS_PORT", "19904")

	tmpDir := t.TempDir()
	sockPath := filepath.Join(tmpDir, "test.sock")

	// Create stale socket file
	f, _ := os.Create(sockPath)
	f.Close()

	d := NewForTesting(sockPath)
	go d.Start()
	defer d.Stop()

	waitForSocket(t, sockPath, 5*time.Second)

	// Should still work (stale socket removed)
	c := client.New(sockPath)
	err := c.Register("1", "test", "/tmp")
	if err != nil {
		t.Fatalf("Register error after stale socket cleanup: %v", err)
	}
}

func TestDaemon_PrunesSessionsWithoutLivePTYOnStart(t *testing.T) {
	t.Setenv("ATTN_WS_PORT", "19924")
	t.Setenv("ATTN_PTY_BACKEND", "embedded")
	sockPath := filepath.Join("/tmp", fmt.Sprintf("attn-prune-%d.sock", time.Now().UnixNano()))

	d := NewForTesting(sockPath)

	nowStr := string(protocol.TimestampNow())
	d.store.Add(&protocol.Session{
		ID:             "stale-session",
		Label:          "stale",
		Agent:          protocol.SessionAgentCodex,
		Directory:      "/tmp/stale",
		State:          protocol.SessionStateWorking,
		StateSince:     nowStr,
		StateUpdatedAt: nowStr,
		LastSeen:       nowStr,
	})

	errCh := make(chan error, 1)
	go func() {
		errCh <- d.Start()
	}()
	defer d.Stop()
	select {
	case err := <-errCh:
		if err != nil {
			t.Fatalf("daemon start error: %v", err)
		}
		t.Fatal("daemon exited unexpectedly during startup")
	case <-time.After(75 * time.Millisecond):
	}
	waitForSocket(t, sockPath, 3*time.Second)

	c := client.New(sockPath)
	sessions, err := c.Query("")
	if err != nil {
		t.Fatalf("Query error: %v", err)
	}
	if len(sessions) != 0 {
		t.Fatalf("expected stale sessions to be pruned on start, got %d", len(sessions))
	}

	warnings := d.getWarnings()
	if len(warnings) == 0 {
		t.Fatal("expected daemon warning for startup stale-session prune")
	}
	if warnings[0].Code != "stale_sessions_pruned" {
		t.Fatalf("warning code = %q, want stale_sessions_pruned", warnings[0].Code)
	}
}

func TestDaemon_Start_SelectsWorkerBackendWhenRequested(t *testing.T) {
	t.Setenv("ATTN_PTY_BACKEND", "worker")
	t.Setenv("ATTN_PTY_SKIP_STARTUP_PROBE", "1")
	t.Setenv("ATTN_WS_PORT", "19926")

	sockPath := filepath.Join(shortTempDir(t), "worker-select.sock")
	d := NewForTesting(sockPath)

	errCh := make(chan error, 1)
	go func() {
		errCh <- d.Start()
	}()
	if !d.waitStarted(3 * time.Second) {
		select {
		case err := <-errCh:
			if err != nil {
				t.Fatalf("daemon start error: %v", err)
			}
			t.Fatal("daemon exited unexpectedly during startup")
		default:
			t.Fatal("daemon did not signal startup")
		}
	}
	defer d.Stop()

	waitForSocket(t, sockPath, 3*time.Second)

	if d.daemonInstanceID == "" {
		t.Fatal("daemon_instance_id should be initialized before backend selection")
	}
	if _, ok := d.ptyBackend.(*ptybackend.WorkerBackend); !ok {
		t.Fatalf("expected worker backend, got %T", d.ptyBackend)
	}
}

func TestDaemon_Start_WorkerProbeFailureFallsBackToEmbedded(t *testing.T) {
	t.Setenv("ATTN_PTY_BACKEND", "worker")
	t.Setenv("ATTN_PTY_SKIP_STARTUP_PROBE", "0")
	t.Setenv("ATTN_PTY_WORKER_BINARY", filepath.Join(t.TempDir(), "missing-attn-binary"))
	t.Setenv("ATTN_WS_PORT", "19936")

	sockPath := filepath.Join(shortTempDir(t), "worker-probe-fallback.sock")
	d := NewForTesting(sockPath)

	errCh := make(chan error, 1)
	go func() {
		errCh <- d.Start()
	}()
	if !d.waitStarted(3 * time.Second) {
		select {
		case err := <-errCh:
			if err != nil {
				t.Fatalf("daemon start error: %v", err)
			}
			t.Fatal("daemon exited unexpectedly during startup")
		default:
			t.Fatal("daemon did not signal startup")
		}
	}
	defer d.Stop()

	if _, ok := d.ptyBackend.(*ptybackend.EmbeddedBackend); !ok {
		t.Fatalf("expected embedded backend after probe failure, got %T", d.ptyBackend)
	}
	hasFallbackWarning := false
	for _, w := range d.getWarnings() {
		if w.Code == warnPTYBackendFallback {
			hasFallbackWarning = true
			break
		}
	}
	if !hasFallbackWarning {
		t.Fatalf("expected %q warning after worker probe failure", warnPTYBackendFallback)
	}
}

func TestDaemon_Start_SelectsEmbeddedBackendWhenRequested(t *testing.T) {
	t.Setenv("ATTN_PTY_BACKEND", "embedded")
	t.Setenv("ATTN_WS_PORT", "19927")

	sockPath := filepath.Join(shortTempDir(t), "embedded-select.sock")
	d := NewForTesting(sockPath)

	errCh := make(chan error, 1)
	go func() {
		errCh <- d.Start()
	}()
	if !d.waitStarted(3 * time.Second) {
		select {
		case err := <-errCh:
			if err != nil {
				t.Fatalf("daemon start error: %v", err)
			}
			t.Fatal("daemon exited unexpectedly during startup")
		default:
			t.Fatal("daemon did not signal startup")
		}
	}
	defer d.Stop()

	waitForSocket(t, sockPath, 3*time.Second)

	if _, ok := d.ptyBackend.(*ptybackend.EmbeddedBackend); !ok {
		t.Fatalf("expected embedded backend, got %T", d.ptyBackend)
	}
}

type fakeWorkerReconcileBackend struct {
	liveIDs []string
	info    map[string]ptybackend.SessionInfo
}

func (b *fakeWorkerReconcileBackend) Spawn(context.Context, ptybackend.SpawnOptions) error {
	return nil
}
func (b *fakeWorkerReconcileBackend) Attach(context.Context, string, string) (ptybackend.AttachInfo, ptybackend.Stream, error) {
	return ptybackend.AttachInfo{}, nil, nil
}
func (b *fakeWorkerReconcileBackend) Input(context.Context, string, []byte) error { return nil }
func (b *fakeWorkerReconcileBackend) Resize(context.Context, string, uint16, uint16) error {
	return nil
}
func (b *fakeWorkerReconcileBackend) Kill(context.Context, string, syscall.Signal) error { return nil }
func (b *fakeWorkerReconcileBackend) Remove(context.Context, string) error               { return nil }
func (b *fakeWorkerReconcileBackend) SessionIDs(context.Context) []string {
	return append([]string(nil), b.liveIDs...)
}
func (b *fakeWorkerReconcileBackend) Recover(context.Context) (ptybackend.RecoveryReport, error) {
	return ptybackend.RecoveryReport{Recovered: len(b.liveIDs)}, nil
}
func (b *fakeWorkerReconcileBackend) Shutdown(context.Context) error { return nil }
func (b *fakeWorkerReconcileBackend) SessionInfo(_ context.Context, sessionID string) (ptybackend.SessionInfo, error) {
	info, ok := b.info[sessionID]
	if !ok {
		return ptybackend.SessionInfo{}, fmt.Errorf("missing info for %s", sessionID)
	}
	return info, nil
}

type fakeDeferredRecoveryBackend struct {
	fakeWorkerReconcileBackend
	mu          sync.Mutex
	reports     []ptybackend.RecoveryReport
	likelyAlive map[string]bool
	likelyErr   map[string]error
}

func (b *fakeDeferredRecoveryBackend) Recover(context.Context) (ptybackend.RecoveryReport, error) {
	b.mu.Lock()
	defer b.mu.Unlock()
	if len(b.reports) == 0 {
		return ptybackend.RecoveryReport{}, nil
	}
	report := b.reports[0]
	b.reports = b.reports[1:]
	return report, nil
}

func (b *fakeDeferredRecoveryBackend) SessionLikelyAlive(_ context.Context, sessionID string) (bool, error) {
	if err, ok := b.likelyErr[sessionID]; ok {
		return false, err
	}
	return b.likelyAlive[sessionID], nil
}

type fakeClearSessionsBackend struct {
	mu               sync.Mutex
	sessionIDs       []string
	recoveredIDs     []string
	recoverCalled    bool
	killed           []string
	removed          []string
	killErrBySession map[string]error
}

func (b *fakeClearSessionsBackend) Spawn(context.Context, ptybackend.SpawnOptions) error { return nil }
func (b *fakeClearSessionsBackend) Attach(context.Context, string, string) (ptybackend.AttachInfo, ptybackend.Stream, error) {
	return ptybackend.AttachInfo{}, nil, nil
}
func (b *fakeClearSessionsBackend) Input(context.Context, string, []byte) error { return nil }
func (b *fakeClearSessionsBackend) Resize(context.Context, string, uint16, uint16) error {
	return nil
}
func (b *fakeClearSessionsBackend) Kill(_ context.Context, sessionID string, _ syscall.Signal) error {
	b.mu.Lock()
	defer b.mu.Unlock()
	b.killed = append(b.killed, sessionID)
	if b.killErrBySession != nil {
		return b.killErrBySession[sessionID]
	}
	return nil
}
func (b *fakeClearSessionsBackend) Remove(_ context.Context, sessionID string) error {
	b.mu.Lock()
	defer b.mu.Unlock()
	b.removed = append(b.removed, sessionID)
	return nil
}
func (b *fakeClearSessionsBackend) SessionIDs(context.Context) []string {
	b.mu.Lock()
	defer b.mu.Unlock()
	return append([]string(nil), b.sessionIDs...)
}
func (b *fakeClearSessionsBackend) Recover(context.Context) (ptybackend.RecoveryReport, error) {
	b.mu.Lock()
	defer b.mu.Unlock()
	b.recoverCalled = true
	b.sessionIDs = append(append([]string(nil), b.sessionIDs...), b.recoveredIDs...)
	return ptybackend.RecoveryReport{Recovered: len(b.recoveredIDs)}, nil
}
func (b *fakeClearSessionsBackend) Shutdown(context.Context) error { return nil }

func TestDaemon_ReconcileSessionsWithWorkerBackend(t *testing.T) {
	d := NewForTesting(filepath.Join(t.TempDir(), "test.sock"))
	now := string(protocol.TimestampNow())
	d.store.Add(&protocol.Session{
		ID:             "live-existing",
		Label:          "existing",
		Agent:          protocol.SessionAgentCodex,
		Directory:      "/tmp/existing",
		State:          protocol.SessionStateWaitingInput,
		StateSince:     now,
		StateUpdatedAt: now,
		LastSeen:       now,
		Recoverable:    protocol.Ptr(true),
	})
	d.store.Add(&protocol.Session{
		ID:             "missing-running",
		Label:          "missing",
		Agent:          protocol.SessionAgentCodex,
		Directory:      "/tmp/missing",
		State:          protocol.SessionStateWorking,
		StateSince:     now,
		StateUpdatedAt: now,
		LastSeen:       now,
	})
	d.store.Add(&protocol.Session{
		ID:             "missing-idle",
		Label:          "missing-idle",
		Agent:          protocol.SessionAgentCodex,
		Directory:      "/tmp/missing-idle",
		State:          protocol.SessionStateIdle,
		StateSince:     now,
		StateUpdatedAt: now,
		LastSeen:       now,
	})
	d.store.Add(&protocol.Session{
		ID:             "live-exited",
		Label:          "live-exited",
		Agent:          protocol.SessionAgentCodex,
		Directory:      "/tmp/live-exited",
		State:          protocol.SessionStateWorking,
		StateSince:     now,
		StateUpdatedAt: now,
		LastSeen:       now,
	})

	d.ptyBackend = &fakeWorkerReconcileBackend{
		liveIDs: []string{"live-existing", "live-new", "live-new-exited", "live-exited", "live-shell"},
		info: map[string]ptybackend.SessionInfo{
			"live-existing": {
				SessionID: "live-existing",
				Agent:     string(protocol.SessionAgentCodex),
				CWD:       "/tmp/existing",
				Running:   true,
				State:     protocol.StateWaitingInput,
			},
			"live-new": {
				SessionID: "live-new",
				Agent:     string(protocol.SessionAgentCopilot),
				CWD:       "/tmp/new-repo",
				Running:   true,
				State:     protocol.StateWorking,
			},
			"live-new-exited": {
				SessionID: "live-new-exited",
				Agent:     string(protocol.SessionAgentCodex),
				CWD:       "/tmp/new-exited",
				Running:   false,
				State:     protocol.StateWorking,
			},
			"live-exited": {
				SessionID: "live-exited",
				Agent:     string(protocol.SessionAgentCodex),
				CWD:       "/tmp/live-exited",
				Running:   false,
				State:     protocol.StateWorking,
			},
			"live-shell": {
				SessionID: "live-shell",
				Agent:     protocol.AgentShellValue,
				CWD:       "/tmp/utility-shell",
				Running:   true,
				State:     protocol.StateWorking,
			},
		},
	}

	report := d.reconcileSessionsWithWorkerBackend(context.Background(), true, time.Time{})
	if report.Created != 2 {
		t.Fatalf("created = %d, want 2", report.Created)
	}
	if report.StateUpdated != 2 {
		t.Fatalf("state_updated = %d, want 2", report.StateUpdated)
	}
	if report.Reaped != 1 {
		t.Fatalf("reaped = %d, want 1", report.Reaped)
	}
	if report.MarkedIdle != 0 {
		t.Fatalf("marked_idle = %d, want 0", report.MarkedIdle)
	}
	if report.SkippedShell != 1 {
		t.Fatalf("skipped_shell = %d, want 1", report.SkippedShell)
	}

	existing := d.store.Get("live-existing")
	if existing == nil {
		t.Fatal("live-existing session missing after reconcile")
	}
	if existing.State != protocol.SessionStateLaunching {
		t.Fatalf("live-existing state = %s, want launching for recovery default", existing.State)
	}
	if protocol.Deref(existing.Recoverable) {
		t.Fatal("live-existing recoverable flag should be cleared once worker is live")
	}

	liveNew := d.store.Get("live-new")
	if liveNew == nil {
		t.Fatal("live-new session was not created from worker metadata")
	}
	if liveNew.Label != "new-repo" {
		t.Fatalf("live-new label = %q, want %q", liveNew.Label, "new-repo")
	}
	if liveNew.Agent != protocol.SessionAgentCopilot {
		t.Fatalf("live-new agent = %s, want %s", liveNew.Agent, protocol.SessionAgentCopilot)
	}
	if liveNew.State != protocol.SessionStateLaunching {
		t.Fatalf("live-new state = %s, want %s", liveNew.State, protocol.SessionStateLaunching)
	}
	liveNewExited := d.store.Get("live-new-exited")
	if liveNewExited == nil {
		t.Fatal("live-new-exited session was not created from worker metadata")
	}
	if liveNewExited.State != protocol.SessionStateIdle {
		t.Fatalf("live-new-exited state = %s, want %s", liveNewExited.State, protocol.SessionStateIdle)
	}
	if liveShell := d.store.Get("live-shell"); liveShell != nil {
		t.Fatalf("live-shell session should not be created from worker metadata, got %#v", liveShell)
	}

	liveExited := d.store.Get("live-exited")
	if liveExited == nil {
		t.Fatal("live-exited session missing after reconcile")
	}
	if liveExited.State != protocol.SessionStateIdle {
		t.Fatalf("live-exited state = %s, want %s", liveExited.State, protocol.SessionStateIdle)
	}

	missingRunning := d.store.Get("missing-running")
	if missingRunning != nil {
		t.Fatal("missing-running session should be reaped (non-claude agent without live PTY)")
	}

	missingIdle := d.store.Get("missing-idle")
	if missingIdle == nil {
		t.Fatal("missing-idle session missing after reconcile")
	}
	if missingIdle.State != protocol.SessionStateIdle {
		t.Fatalf("missing-idle state = %s, want idle", missingIdle.State)
	}
}

func TestDaemon_ReconcileSessionsWithWorkerBackend_ClaudeSessionsRecoverable(t *testing.T) {
	d := NewForTesting(filepath.Join(t.TempDir(), "test.sock"))
	now := string(protocol.TimestampNow())

	// Claude session without live PTY should be marked recoverable
	d.store.Add(&protocol.Session{
		ID:             "claude-stale",
		Label:          "claude-stale",
		Agent:          protocol.SessionAgentClaude,
		Directory:      "/tmp/claude-stale",
		State:          protocol.SessionStateWorking,
		StateSince:     now,
		StateUpdatedAt: now,
		LastSeen:       now,
	})

	// Codex session without live PTY should be reaped
	d.store.Add(&protocol.Session{
		ID:             "codex-stale",
		Label:          "codex-stale",
		Agent:          protocol.SessionAgentCodex,
		Directory:      "/tmp/codex-stale",
		State:          protocol.SessionStateWorking,
		StateSince:     now,
		StateUpdatedAt: now,
		LastSeen:       now,
	})

	// Copilot session without live PTY should be reaped
	d.store.Add(&protocol.Session{
		ID:             "copilot-stale",
		Label:          "copilot-stale",
		Agent:          protocol.SessionAgentCopilot,
		Directory:      "/tmp/copilot-stale",
		State:          protocol.SessionStateWorking,
		StateSince:     now,
		StateUpdatedAt: now,
		LastSeen:       now,
	})

	d.ptyBackend = &fakeWorkerReconcileBackend{
		liveIDs: nil,
		info:    map[string]ptybackend.SessionInfo{},
	}

	report := d.reconcileSessionsWithWorkerBackend(context.Background(), true, time.Time{})

	if report.MarkedRecoverable != 1 {
		t.Fatalf("marked_recoverable = %d, want 1", report.MarkedRecoverable)
	}
	if report.Reaped != 2 {
		t.Fatalf("reaped = %d, want 2", report.Reaped)
	}

	// Claude session should be idle + recoverable
	claudeSession := d.store.Get("claude-stale")
	if claudeSession == nil {
		t.Fatal("claude-stale session should not be reaped")
	}
	if claudeSession.State != protocol.SessionStateIdle {
		t.Fatalf("claude-stale state = %s, want idle", claudeSession.State)
	}
	if !protocol.Deref(claudeSession.Recoverable) {
		t.Fatal("claude-stale should be marked recoverable")
	}

	// Non-claude sessions should be removed
	if d.store.Get("codex-stale") != nil {
		t.Fatal("codex-stale session should be reaped")
	}
	if d.store.Get("copilot-stale") != nil {
		t.Fatal("copilot-stale session should be reaped")
	}
}

func TestDaemon_RunDeferredWorkerReconciliationForcesIdleDemotion(t *testing.T) {
	d := NewForTesting(filepath.Join(t.TempDir(), "test.sock"))
	now := string(protocol.TimestampNow())
	d.store.Add(&protocol.Session{
		ID:             "stale-running",
		Label:          "stale-running",
		Agent:          protocol.SessionAgentCodex,
		Directory:      "/tmp/stale-running",
		State:          protocol.SessionStateWorking,
		StateSince:     now,
		StateUpdatedAt: now,
		LastSeen:       now,
	})
	d.ptyBackend = &fakeDeferredRecoveryBackend{
		fakeWorkerReconcileBackend: fakeWorkerReconcileBackend{
			liveIDs: nil,
			info:    map[string]ptybackend.SessionInfo{},
		},
		reports: []ptybackend.RecoveryReport{
			{Missing: 1},
		},
	}

	d.runDeferredWorkerReconciliation(1, 0, time.Now())

	session := d.store.Get("stale-running")
	if session != nil {
		t.Fatal("stale-running session should be reaped (non-claude agent without live PTY)")
	}

	warnings := d.getWarnings()
	hasPartial := false
	for _, w := range warnings {
		if w.Code == "worker_recovery_partial" && strings.Contains(w.Message, "Forced stale-session reconciliation") {
			hasPartial = true
			break
		}
	}
	if !hasPartial {
		t.Fatalf("expected forced reconciliation warning, got %+v", warnings)
	}
}

func TestDaemon_RunDeferredWorkerReconciliation_BroadcastsSessionsUpdatedOnChange(t *testing.T) {
	d := NewForTesting(filepath.Join(t.TempDir(), "test.sock"))
	now := string(protocol.TimestampNow())
	d.store.Add(&protocol.Session{
		ID:             "stale-running",
		Label:          "stale-running",
		Agent:          protocol.SessionAgentCodex,
		Directory:      "/tmp/stale-running",
		State:          protocol.SessionStateWorking,
		StateSince:     now,
		StateUpdatedAt: now,
		LastSeen:       now,
	})
	d.ptyBackend = &fakeDeferredRecoveryBackend{
		fakeWorkerReconcileBackend: fakeWorkerReconcileBackend{
			liveIDs: nil,
			info:    map[string]ptybackend.SessionInfo{},
		},
		reports: []ptybackend.RecoveryReport{
			{Missing: 1},
		},
	}

	broadcasts := 0
	d.wsHub.broadcastListener = func(event *protocol.WebSocketEvent) {
		if event != nil && event.Event == protocol.EventSessionsUpdated {
			broadcasts++
		}
	}

	d.runDeferredWorkerReconciliation(1, 0, time.Now())

	if broadcasts == 0 {
		t.Fatal("expected deferred reconciliation to broadcast sessions_updated after state changes")
	}
}

func TestDaemon_ReconcileSessionsWithWorkerBackend_PreservesLikelyAliveSessions(t *testing.T) {
	d := NewForTesting(filepath.Join(t.TempDir(), "test.sock"))
	now := string(protocol.TimestampNow())
	d.store.Add(&protocol.Session{
		ID:             "stale-running",
		Label:          "stale-running",
		Agent:          protocol.SessionAgentCodex,
		Directory:      "/tmp/stale-running",
		State:          protocol.SessionStateWorking,
		StateSince:     now,
		StateUpdatedAt: now,
		LastSeen:       now,
	})
	d.ptyBackend = &fakeDeferredRecoveryBackend{
		fakeWorkerReconcileBackend: fakeWorkerReconcileBackend{
			liveIDs: nil,
			info:    map[string]ptybackend.SessionInfo{},
		},
		likelyAlive: map[string]bool{
			"stale-running": true,
		},
	}

	report := d.reconcileSessionsWithWorkerBackend(context.Background(), true, time.Time{})
	if report.MarkedIdle != 0 {
		t.Fatalf("marked_idle = %d, want 0", report.MarkedIdle)
	}
	if report.LikelyAlive != 1 {
		t.Fatalf("likely_alive = %d, want 1", report.LikelyAlive)
	}
	session := d.store.Get("stale-running")
	if session == nil {
		t.Fatal("stale-running session missing")
	}
	if session.State != protocol.SessionStateWorking {
		t.Fatalf("state = %q, want %q", session.State, protocol.SessionStateWorking)
	}
}

func TestDaemon_ReconcileSessionsWithWorkerBackend_SkipsIdleDemotionOnLivenessProbeError(t *testing.T) {
	d := NewForTesting(filepath.Join(t.TempDir(), "test.sock"))
	now := string(protocol.TimestampNow())
	d.store.Add(&protocol.Session{
		ID:             "stale-running",
		Label:          "stale-running",
		Agent:          protocol.SessionAgentCodex,
		Directory:      "/tmp/stale-running",
		State:          protocol.SessionStateWorking,
		StateSince:     now,
		StateUpdatedAt: now,
		LastSeen:       now,
	})
	d.ptyBackend = &fakeDeferredRecoveryBackend{
		fakeWorkerReconcileBackend: fakeWorkerReconcileBackend{
			liveIDs: nil,
			info:    map[string]ptybackend.SessionInfo{},
		},
		likelyErr: map[string]error{
			"stale-running": errors.New("probe timeout"),
		},
	}

	report := d.reconcileSessionsWithWorkerBackend(context.Background(), true, time.Time{})
	if report.MarkedIdle != 0 {
		t.Fatalf("marked_idle = %d, want 0", report.MarkedIdle)
	}
	if report.LivenessUnknown != 1 {
		t.Fatalf("liveness_unknown = %d, want 1", report.LivenessUnknown)
	}
	session := d.store.Get("stale-running")
	if session == nil {
		t.Fatal("stale-running session missing")
	}
	if session.State != protocol.SessionStateWorking {
		t.Fatalf("state = %q, want %q", session.State, protocol.SessionStateWorking)
	}
}

func TestDaemon_ReconcileSessionsWithWorkerBackend_SkipsIdleDemotionOnIncompleteRecovery(t *testing.T) {
	d := NewForTesting(filepath.Join(t.TempDir(), "test.sock"))
	now := string(protocol.TimestampNow())
	d.store.Add(&protocol.Session{
		ID:             "missing-running",
		Label:          "missing",
		Agent:          protocol.SessionAgentCodex,
		Directory:      "/tmp/missing",
		State:          protocol.SessionStateWorking,
		StateSince:     now,
		StateUpdatedAt: now,
		LastSeen:       now,
	})

	d.ptyBackend = &fakeWorkerReconcileBackend{
		liveIDs: nil,
		info:    map[string]ptybackend.SessionInfo{},
	}

	report := d.reconcileSessionsWithWorkerBackend(context.Background(), false, time.Time{})
	if report.MarkedIdle != 0 {
		t.Fatalf("marked_idle = %d, want 0", report.MarkedIdle)
	}
	if report.SkippedIdle != 1 {
		t.Fatalf("skipped_idle = %d, want 1", report.SkippedIdle)
	}
	session := d.store.Get("missing-running")
	if session == nil {
		t.Fatal("missing-running session missing after reconcile")
	}
	if session.State != protocol.SessionStateWorking {
		t.Fatalf("missing-running state = %s, want working", session.State)
	}
}

func TestDaemon_ReconcileSessionsWithWorkerBackend_SkipsRecentlyUpdatedSessions(t *testing.T) {
	d := NewForTesting(filepath.Join(t.TempDir(), "test.sock"))
	cutoff := time.Now().Add(-time.Minute).Truncate(time.Second)
	updatedAt := cutoff.Add(10 * time.Second)
	timestamp := protocol.NewTimestamp(updatedAt).String()

	d.store.Add(&protocol.Session{
		ID:             "recent-running",
		Label:          "recent",
		Agent:          protocol.SessionAgentCodex,
		Directory:      "/tmp/recent",
		State:          protocol.SessionStateWorking,
		StateSince:     timestamp,
		StateUpdatedAt: timestamp,
		LastSeen:       timestamp,
	})

	d.ptyBackend = &fakeWorkerReconcileBackend{
		liveIDs: nil,
		info:    map[string]ptybackend.SessionInfo{},
	}

	report := d.reconcileSessionsWithWorkerBackend(context.Background(), true, cutoff)
	if report.MarkedIdle != 0 {
		t.Fatalf("marked_idle = %d, want 0", report.MarkedIdle)
	}
	if report.SkippedRecent != 1 {
		t.Fatalf("skipped_recent = %d, want 1", report.SkippedRecent)
	}
	session := d.store.Get("recent-running")
	if session == nil {
		t.Fatal("recent-running session missing after reconcile")
	}
	if session.State != protocol.SessionStateWorking {
		t.Fatalf("recent-running state = %s, want working", session.State)
	}
}

func TestSessionStateFromRecoveredInfo(t *testing.T) {
	tests := []struct {
		name string
		info ptybackend.SessionInfo
		want protocol.SessionState
	}{
		{
			name: "not running is idle",
			info: ptybackend.SessionInfo{Running: false, State: protocol.StateWorking},
			want: protocol.SessionStateIdle,
		},
		{
			name: "waiting input",
			info: ptybackend.SessionInfo{Running: true, Agent: string(protocol.SessionAgentClaude), State: protocol.StateWaitingInput},
			want: protocol.SessionStateWaitingInput,
		},
		{
			name: "codex waiting input normalizes to launching",
			info: ptybackend.SessionInfo{Running: true, Agent: string(protocol.SessionAgentCodex), State: protocol.StateWaitingInput},
			want: protocol.SessionStateLaunching,
		},
		{
			name: "pending approval",
			info: ptybackend.SessionInfo{Running: true, State: protocol.StatePendingApproval},
			want: protocol.SessionStatePendingApproval,
		},
		{
			name: "explicit idle running session normalizes to launching",
			info: ptybackend.SessionInfo{Running: true, Agent: string(protocol.SessionAgentClaude), State: protocol.StateIdle},
			want: protocol.SessionStateLaunching,
		},
		{
			name: "copilot explicit idle normalizes to launching",
			info: ptybackend.SessionInfo{Running: true, Agent: string(protocol.SessionAgentCopilot), State: protocol.StateIdle},
			want: protocol.SessionStateLaunching,
		},
		{
			name: "default working normalizes to launching",
			info: ptybackend.SessionInfo{Running: true, State: protocol.StateWorking},
			want: protocol.SessionStateLaunching,
		},
	}
	for _, tt := range tests {
		t.Run(tt.name, func(t *testing.T) {
			got := sessionStateFromRecoveredInfo(tt.info)
			if got != tt.want {
				t.Fatalf("sessionStateFromRecoveredInfo() = %s, want %s", got, tt.want)
			}
		})
	}
}

type fakeOutputStream struct {
	mu         sync.Mutex
	events     chan ptybackend.OutputEvent
	closeCount int
	once       sync.Once
}

func newFakeOutputStream() *fakeOutputStream {
	return &fakeOutputStream{events: make(chan ptybackend.OutputEvent, 8)}
}

func (s *fakeOutputStream) Events() <-chan ptybackend.OutputEvent { return s.events }
func (s *fakeOutputStream) Close() error {
	s.once.Do(func() {
		s.mu.Lock()
		s.closeCount++
		s.mu.Unlock()
		close(s.events)
	})
	return nil
}

func (s *fakeOutputStream) ClosedCount() int {
	s.mu.Lock()
	defer s.mu.Unlock()
	return s.closeCount
}

type fakeAttachBackend struct {
	mu      sync.Mutex
	streams []*fakeOutputStream
	failErr error
}

func (b *fakeAttachBackend) Spawn(context.Context, ptybackend.SpawnOptions) error { return nil }
func (b *fakeAttachBackend) Attach(context.Context, string, string) (ptybackend.AttachInfo, ptybackend.Stream, error) {
	b.mu.Lock()
	if b.failErr != nil {
		err := b.failErr
		b.mu.Unlock()
		return ptybackend.AttachInfo{}, nil, err
	}
	b.mu.Unlock()

	stream := newFakeOutputStream()
	b.mu.Lock()
	b.streams = append(b.streams, stream)
	b.mu.Unlock()
	return ptybackend.AttachInfo{Running: true}, stream, nil
}
func (b *fakeAttachBackend) Input(context.Context, string, []byte) error { return nil }
func (b *fakeAttachBackend) Resize(context.Context, string, uint16, uint16) error {
	return nil
}
func (b *fakeAttachBackend) Kill(context.Context, string, syscall.Signal) error { return nil }
func (b *fakeAttachBackend) Remove(context.Context, string) error               { return nil }
func (b *fakeAttachBackend) SessionIDs(context.Context) []string                { return nil }
func (b *fakeAttachBackend) Recover(context.Context) (ptybackend.RecoveryReport, error) {
	return ptybackend.RecoveryReport{}, nil
}
func (b *fakeAttachBackend) Shutdown(context.Context) error { return nil }

func (b *fakeAttachBackend) Streams() []*fakeOutputStream {
	b.mu.Lock()
	defer b.mu.Unlock()
	out := make([]*fakeOutputStream, len(b.streams))
	copy(out, b.streams)
	return out
}

func (b *fakeAttachBackend) FailNextAttach(err error) {
	b.mu.Lock()
	defer b.mu.Unlock()
	b.failErr = err
}

func TestDaemon_ForwardPTYStreamEvents_ClosesStreamOnSendFailure(t *testing.T) {
	d := NewForTesting(filepath.Join(t.TempDir(), "test.sock"))
	client := &wsClient{
		send:            make(chan outboundMessage, 1),
		attachedStreams: make(map[string]ptybackend.Stream),
	}

	// Fill send buffer so sendOutbound() fails.
	client.send <- outboundMessage{kind: messageKindText, payload: []byte(`{\"event\":\"noop\"}`)}

	stream := newFakeOutputStream()
	client.attachedStreams["sess-1"] = stream

	done := make(chan struct{})
	go func() {
		d.forwardPTYStreamEvents(client, "sess-1", stream)
		close(done)
	}()

	stream.events <- ptybackend.OutputEvent{
		Kind: ptybackend.OutputEventKindOutput,
		Data: []byte("hello"),
		Seq:  1,
	}

	select {
	case <-done:
	case <-time.After(2 * time.Second):
		t.Fatal("forwardPTYStreamEvents did not exit after send failure")
	}

	if stream.ClosedCount() == 0 {
		t.Fatal("stream should be closed after send failure")
	}
}

func TestDaemon_HandleAttachSession_ReattachFailureKeepsExistingStream(t *testing.T) {
	d := NewForTesting(filepath.Join(t.TempDir(), "test.sock"))
	backend := &fakeAttachBackend{}
	d.ptyBackend = backend

	client := &wsClient{
		send:            make(chan outboundMessage, 4),
		attachedStreams: make(map[string]ptybackend.Stream),
	}

	d.handleAttachSession(client, &protocol.AttachSessionMessage{Cmd: protocol.CmdAttachSession, ID: "sess-1"})
	firstMsg := <-client.send
	var first protocol.AttachResultMessage
	if err := json.Unmarshal(firstMsg.payload, &first); err != nil {
		t.Fatalf("decode first attach result: %v", err)
	}
	if !first.Success {
		t.Fatalf("first attach success = false, err=%q", protocol.Deref(first.Error))
	}

	streams := backend.Streams()
	if len(streams) != 1 {
		t.Fatalf("streams len = %d, want 1", len(streams))
	}
	firstStream := streams[0]
	if got := firstStream.ClosedCount(); got != 0 {
		t.Fatalf("first stream closed count = %d, want 0", got)
	}

	backend.FailNextAttach(errors.New("temporary attach failure"))
	d.handleAttachSession(client, &protocol.AttachSessionMessage{Cmd: protocol.CmdAttachSession, ID: "sess-1"})
	secondMsg := <-client.send
	var second protocol.AttachResultMessage
	if err := json.Unmarshal(secondMsg.payload, &second); err != nil {
		t.Fatalf("decode second attach result: %v", err)
	}
	if second.Success {
		t.Fatal("second attach should fail")
	}

	client.attachMu.Lock()
	current := client.attachedStreams["sess-1"]
	client.attachMu.Unlock()
	if current == nil {
		t.Fatal("existing stream should remain attached after reattach failure")
	}
	if current != firstStream {
		t.Fatal("attached stream changed after reattach failure")
	}
	if got := firstStream.ClosedCount(); got != 0 {
		t.Fatalf("first stream closed count = %d, want 0 after failed reattach", got)
	}
}

func TestDaemon_HandleAttachSession_ReattachClosesOldStream(t *testing.T) {
	d := NewForTesting(filepath.Join(t.TempDir(), "test.sock"))
	backend := &fakeAttachBackend{}
	d.ptyBackend = backend
	client := &wsClient{
		send:            make(chan outboundMessage, 8),
		attachedStreams: make(map[string]ptybackend.Stream),
	}

	d.handleAttachSession(client, &protocol.AttachSessionMessage{ID: "sess-1"})
	d.handleAttachSession(client, &protocol.AttachSessionMessage{ID: "sess-1"})

	streams := backend.Streams()
	if len(streams) != 2 {
		t.Fatalf("attach streams = %d, want 2", len(streams))
	}
	if streams[0].ClosedCount() == 0 {
		t.Fatal("first stream should be closed on reattach")
	}

	client.attachMu.Lock()
	current, ok := client.attachedStreams["sess-1"]
	client.attachMu.Unlock()
	if !ok {
		t.Fatal("expected current attached stream for sess-1")
	}
	if current != streams[1] {
		t.Fatal("attached stream should be the most recent stream")
	}

	d.detachSession(client, "sess-1")
}

func TestDaemon_NewAddsWarningWhenPersistenceFallsBackToMemory(t *testing.T) {
	t.Setenv("ATTN_DB_PATH", filepath.Join("/dev/null", "attn.db"))

	d := New(filepath.Join(t.TempDir(), "test.sock"))
	defer d.store.Close()

	warnings := d.getWarnings()
	if len(warnings) == 0 {
		t.Fatal("expected warning when DB open fails and daemon falls back to in-memory")
	}

	found := false
	for _, warning := range warnings {
		if warning.Code != "persistence_degraded" {
			continue
		}
		found = true
		if !strings.Contains(warning.Message, "Running in-memory only") {
			t.Fatalf("warning message missing in-memory note: %q", warning.Message)
		}
		if !strings.Contains(warning.Message, "See daemon log in "+config.LogPath()) {
			t.Fatalf("warning message missing daemon log path: %q", warning.Message)
		}
		if !strings.Contains(warning.Message, "/dev/null/attn.db") {
			t.Fatalf("warning message missing DB path: %q", warning.Message)
		}
	}
	if !found {
		t.Fatalf("expected persistence_degraded warning, got: %+v", warnings)
	}

	now := string(protocol.TimestampNow())
	d.store.Add(&protocol.Session{
		ID:             "fallback-store-session",
		Label:          "fallback-store-session",
		Agent:          protocol.SessionAgentCodex,
		Directory:      t.TempDir(),
		State:          protocol.SessionStateWorking,
		StateSince:     now,
		StateUpdatedAt: now,
		LastSeen:       now,
	})
	if got := d.store.Get("fallback-store-session"); got == nil {
		t.Fatal("expected in-memory fallback store to remain usable")
	}
}

func TestDaemon_HandleClientMessage_ClearWarnings(t *testing.T) {
	d := NewForTesting(filepath.Join(t.TempDir(), "test.sock"))
	d.addWarning("one", "first warning")
	d.addWarning("two", "second warning")
	if got := len(d.getWarnings()); got != 2 {
		t.Fatalf("warnings before clear = %d, want 2", got)
	}

	client := &wsClient{}
	d.handleClientMessage(client, []byte(`{"cmd":"clear_warnings"}`))

	if got := len(d.getWarnings()); got != 0 {
		t.Fatalf("warnings after clear = %d, want 0", got)
	}
}

func TestDaemon_AddWarning_DedupesByCodeAndMessage(t *testing.T) {
	d := NewForTesting(filepath.Join(t.TempDir(), "test.sock"))
	d.addWarning("worker_recovery_partial", "first")
	d.addWarning("worker_recovery_partial", "second")
	d.addWarning("worker_recovery_partial", "second")

	warnings := d.getWarnings()
	if len(warnings) != 2 {
		t.Fatalf("warnings len = %d, want 2", len(warnings))
	}
}

func TestDaemon_ClearWarningsNotReplayedInInitialState(t *testing.T) {
	d := NewForTesting(filepath.Join(t.TempDir(), "test.sock"))
	d.addWarning("stale_sessions_pruned", "Removed 1 stale sessions from a previous daemon run because no live PTY was found.")

	client := &wsClient{
		send: make(chan outboundMessage, 4),
	}

	d.sendInitialState(client)
	first := <-client.send
	var firstEvent protocol.WebSocketEvent
	if err := json.Unmarshal(first.payload, &firstEvent); err != nil {
		t.Fatalf("decode first initial_state: %v", err)
	}
	if firstEvent.Event != protocol.EventInitialState {
		t.Fatalf("first event = %q, want %q", firstEvent.Event, protocol.EventInitialState)
	}
	if got := len(firstEvent.Warnings); got != 1 {
		t.Fatalf("first initial_state warnings = %d, want 1", got)
	}

	d.handleClientMessage(client, []byte(`{"cmd":"clear_warnings"}`))

	d.sendInitialState(client)
	second := <-client.send
	var secondEvent protocol.WebSocketEvent
	if err := json.Unmarshal(second.payload, &secondEvent); err != nil {
		t.Fatalf("decode second initial_state: %v", err)
	}
	if secondEvent.Event != protocol.EventInitialState {
		t.Fatalf("second event = %q, want %q", secondEvent.Event, protocol.EventInitialState)
	}
	if got := len(secondEvent.Warnings); got != 0 {
		t.Fatalf("second initial_state warnings = %d, want 0", got)
	}
}

func TestDaemon_InitialState_IncludesDaemonInstanceID(t *testing.T) {
	d := NewForTesting(filepath.Join(t.TempDir(), "test.sock"))
	d.daemonInstanceID = "d-aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa"

	client := &wsClient{
		send:            make(chan outboundMessage, 2),
		attachedStreams: make(map[string]ptybackend.Stream),
	}

	d.sendInitialState(client)
	msg := <-client.send

	var initial protocol.InitialStateMessage
	if err := json.Unmarshal(msg.payload, &initial); err != nil {
		t.Fatalf("decode initial_state: %v", err)
	}
	if protocol.Deref(initial.DaemonInstanceID) != d.daemonInstanceID {
		t.Fatalf("daemon_instance_id = %q, want %q", protocol.Deref(initial.DaemonInstanceID), d.daemonInstanceID)
	}
}

func TestDaemon_RecoveryBarrier_BlocksPTYCommands(t *testing.T) {
	d := NewForTesting(filepath.Join(t.TempDir(), "test.sock"))
	d.setRecovering(true)

	client := &wsClient{
		send:            make(chan outboundMessage, 2),
		attachedStreams: make(map[string]ptybackend.Stream),
	}

	d.handleClientMessage(client, []byte(`{"cmd":"attach_session","id":"sess-1"}`))

	msg := <-client.send
	var event protocol.WebSocketEvent
	if err := json.Unmarshal(msg.payload, &event); err != nil {
		t.Fatalf("decode command_error: %v", err)
	}
	if event.Event != protocol.EventCommandError {
		t.Fatalf("event = %q, want %q", event.Event, protocol.EventCommandError)
	}
	if protocol.Deref(event.Cmd) != protocol.CmdAttachSession {
		t.Fatalf("cmd = %q, want %q", protocol.Deref(event.Cmd), protocol.CmdAttachSession)
	}
	if protocol.Deref(event.Error) != "daemon_recovering" {
		t.Fatalf("error = %q, want %q", protocol.Deref(event.Error), "daemon_recovering")
	}
}

func TestDaemon_RecoveryBarrier_BlocksClearSessions(t *testing.T) {
	d := NewForTesting(filepath.Join(t.TempDir(), "test.sock"))
	d.setRecovering(true)

	now := string(protocol.TimestampNow())
	d.store.Add(&protocol.Session{
		ID:             "sess-1",
		Label:          "sess-1",
		Agent:          protocol.SessionAgentCodex,
		Directory:      "/tmp/sess-1",
		State:          protocol.SessionStateWorking,
		StateSince:     now,
		StateUpdatedAt: now,
		LastSeen:       now,
	})

	client := &wsClient{
		send:            make(chan outboundMessage, 2),
		attachedStreams: make(map[string]ptybackend.Stream),
	}

	d.handleClientMessage(client, []byte(`{"cmd":"clear_sessions"}`))

	msg := <-client.send
	var event protocol.WebSocketEvent
	if err := json.Unmarshal(msg.payload, &event); err != nil {
		t.Fatalf("decode command_error: %v", err)
	}
	if event.Event != protocol.EventCommandError {
		t.Fatalf("event = %q, want %q", event.Event, protocol.EventCommandError)
	}
	if protocol.Deref(event.Cmd) != protocol.CmdClearSessions {
		t.Fatalf("cmd = %q, want %q", protocol.Deref(event.Cmd), protocol.CmdClearSessions)
	}
	if protocol.Deref(event.Error) != "daemon_recovering" {
		t.Fatalf("error = %q, want %q", protocol.Deref(event.Error), "daemon_recovering")
	}
	if got := len(d.store.List("")); got != 1 {
		t.Fatalf("store sessions = %d, want 1 (clear should be blocked during recovery)", got)
	}
}

func TestDaemon_ClearAllSessions_RecoversAndTerminatesKnownSessions(t *testing.T) {
	d := NewForTesting(filepath.Join(t.TempDir(), "test.sock"))
	now := string(protocol.TimestampNow())
	d.store.Add(&protocol.Session{
		ID:             "store-session",
		Label:          "store-session",
		Agent:          protocol.SessionAgentCodex,
		Directory:      "/tmp/store-session",
		State:          protocol.SessionStateWorking,
		StateSince:     now,
		StateUpdatedAt: now,
		LastSeen:       now,
	})

	backend := &fakeClearSessionsBackend{
		sessionIDs:   []string{"attached-session"},
		recoveredIDs: []string{"registry-only-session"},
	}
	d.ptyBackend = backend

	d.clearAllSessions()

	if got := len(d.store.List("")); got != 0 {
		t.Fatalf("store sessions = %d, want 0", got)
	}

	backend.mu.Lock()
	recoverCalled := backend.recoverCalled
	killed := append([]string(nil), backend.killed...)
	removed := append([]string(nil), backend.removed...)
	backend.mu.Unlock()

	if !recoverCalled {
		t.Fatal("expected clearAllSessions to call backend Recover()")
	}
	expectKilled := map[string]bool{
		"store-session":         false,
		"attached-session":      false,
		"registry-only-session": false,
	}
	for _, id := range killed {
		if _, ok := expectKilled[id]; ok {
			expectKilled[id] = true
		}
	}
	for id, seen := range expectKilled {
		if !seen {
			t.Fatalf("expected kill for %s, got kills=%v", id, killed)
		}
	}
	expectRemoved := map[string]bool{
		"store-session":         false,
		"attached-session":      false,
		"registry-only-session": false,
	}
	for _, id := range removed {
		if _, ok := expectRemoved[id]; ok {
			expectRemoved[id] = true
		}
	}
	for id, seen := range expectRemoved {
		if !seen {
			t.Fatalf("expected remove for %s, got removes=%v", id, removed)
		}
	}
}

func TestDaemon_RecoveryBarrier_DefersInitialState(t *testing.T) {
	d := NewForTesting(filepath.Join(t.TempDir(), "test.sock"))
	d.daemonInstanceID = "d-aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa"
	d.setRecovering(true)

	client := &wsClient{
		send:            make(chan outboundMessage, 2),
		attachedStreams: make(map[string]ptybackend.Stream),
	}

	d.scheduleInitialState(client)
	select {
	case <-client.send:
		t.Fatal("initial_state was sent while daemon was recovering")
	default:
	}

	d.setRecovering(false)
	select {
	case msg := <-client.send:
		var initial protocol.InitialStateMessage
		if err := json.Unmarshal(msg.payload, &initial); err != nil {
			t.Fatalf("decode deferred initial_state: %v", err)
		}
		if initial.Event != protocol.EventInitialState {
			t.Fatalf("event = %q, want %q", initial.Event, protocol.EventInitialState)
		}
	case <-time.After(1 * time.Second):
		t.Fatal("timed out waiting for deferred initial_state")
	}
}

func TestDaemon_HealthEndpoint(t *testing.T) {
	tmpDir := t.TempDir()
	sockPath := filepath.Join(tmpDir, "test.sock")

	// Use unique port
	wsPort := "19851"
	os.Setenv("ATTN_WS_PORT", wsPort)
	defer os.Unsetenv("ATTN_WS_PORT")

	d := NewForTesting(sockPath)
	go d.Start()
	defer d.Stop()

	waitForSocket(t, sockPath, 5*time.Second)

	// Register a session to verify it's counted
	c := client.New(sockPath)
	c.Register("test-1", "test", "/tmp")

	// Poll the health endpoint until the HTTP server is ready
	healthURL := "http://127.0.0.1:" + wsPort + "/health"
	var resp *http.Response
	deadline := time.Now().Add(5 * time.Second)
	for time.Now().Before(deadline) {
		r, err := http.Get(healthURL)
		if err == nil {
			resp = r
			break
		}
		time.Sleep(50 * time.Millisecond)
	}
	if resp == nil {
		t.Fatalf("Health endpoint not ready after 5s")
	}
	defer resp.Body.Close()

	if resp.StatusCode != 200 {
		t.Errorf("Health status = %d, want 200", resp.StatusCode)
	}

	var health map[string]interface{}
	if err := json.NewDecoder(resp.Body).Decode(&health); err != nil {
		t.Fatalf("Decode error: %v", err)
	}

	if health["status"] != "ok" {
		t.Errorf("status = %v, want ok", health["status"])
	}
	if health["protocol"] != protocol.ProtocolVersion {
		t.Errorf("protocol = %v, want %s", health["protocol"], protocol.ProtocolVersion)
	}
	if daemonID, ok := health["daemon_instance_id"].(string); !ok || daemonID == "" {
		t.Errorf("daemon_instance_id = %v, want non-empty string", health["daemon_instance_id"])
	}
	// sessions should be 1.0 (float64 from JSON)
	if sessions, ok := health["sessions"].(float64); !ok || sessions != 1 {
		t.Errorf("sessions = %v, want 1", health["sessions"])
	}
}

func TestDaemon_SettingsValidation(t *testing.T) {
	// Test the validateSetting function directly
	d := &Daemon{}

	tests := []struct {
		name    string
		key     string
		value   string
		wantErr bool
	}{
		{"valid projects_directory", "projects_directory", t.TempDir(), false},
		{"valid new_session_agent codex", "new_session_agent", "codex", false},
		{"valid new_session_agent claude", "new_session_agent", "claude", false},
		{"valid new_session_agent copilot", "new_session_agent", "copilot", false},
		{"empty new_session_agent", "new_session_agent", "", false},
		{"empty claude_executable", "claude_executable", "", false},
		{"empty codex_executable", "codex_executable", "", false},
		{"empty copilot_executable", "copilot_executable", "", false},
		{"invalid claude_executable", "claude_executable", "not-a-real-binary-123", true},
		{"invalid new_session_agent", "new_session_agent", "gpt", true},
		{"invalid key", "unknown_setting", "value", true},
		{"empty projects_directory", "projects_directory", "", true},
		{"relative path", "projects_directory", "relative/path", true},
	}

	for _, tt := range tests {
		t.Run(tt.name, func(t *testing.T) {
			err := d.validateSetting(tt.key, tt.value)
			if (err != nil) != tt.wantErr {
				t.Errorf("validateSetting(%q, %q) error = %v, wantErr %v", tt.key, tt.value, err, tt.wantErr)
			}
		})
	}
}

func TestDaemon_SettingsWithAgentAvailability(t *testing.T) {
	t.Setenv("PATH", "")
	d := &Daemon{store: store.New()}
	d.store.SetSetting(SettingNewSessionAgent, "codex")

	settings := d.settingsWithAgentAvailability()
	if got := settings[SettingNewSessionAgent]; got != "codex" {
		t.Fatalf("settings[%s] = %v, want codex", SettingNewSessionAgent, got)
	}
	if got := settings[SettingClaudeAvailable]; got != "false" {
		t.Fatalf("settings[%s] = %v, want false", SettingClaudeAvailable, got)
	}
	if got := settings[SettingCodexAvailable]; got != "false" {
		t.Fatalf("settings[%s] = %v, want false", SettingCodexAvailable, got)
	}
	if got := settings[SettingCopilotAvailable]; got != "false" {
		t.Fatalf("settings[%s] = %v, want false", SettingCopilotAvailable, got)
	}
	if got := settings[SettingPTYBackendMode]; got != "unknown" {
		t.Fatalf("settings[%s] = %v, want unknown", SettingPTYBackendMode, got)
	}

	tmp := t.TempDir()
	custom := filepath.Join(tmp, "custom-codex")
	if err := os.WriteFile(custom, []byte("#!/bin/sh\nexit 0\n"), 0755); err != nil {
		t.Fatalf("write custom executable: %v", err)
	}
	t.Setenv("PATH", tmp)
	d.store.SetSetting(SettingCodexExecutable, "custom-codex")

	settings = d.settingsWithAgentAvailability()
	if got := settings[SettingCodexAvailable]; got != "true" {
		t.Fatalf("settings[%s] = %v, want true", SettingCodexAvailable, got)
	}
	if got := settings[SettingClaudeAvailable]; got != "false" {
		t.Fatalf("settings[%s] = %v, want false", SettingClaudeAvailable, got)
	}
	if got := settings[SettingCopilotAvailable]; got != "false" {
		t.Fatalf("settings[%s] = %v, want false", SettingCopilotAvailable, got)
	}
}

func TestDaemon_SettingsIncludePTYBackendMode(t *testing.T) {
	d := NewForTesting(filepath.Join(t.TempDir(), "daemon.sock"))

	settings := d.settingsWithAgentAvailability()
	if got := settings[SettingPTYBackendMode]; got != "embedded" {
		t.Fatalf("settings[%s] = %v, want embedded", SettingPTYBackendMode, got)
	}

	workerBackend, err := ptybackend.NewWorker(ptybackend.WorkerBackendConfig{
		DataRoot:         t.TempDir(),
		DaemonInstanceID: "d-aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa",
		BinaryPath:       "/bin/true",
	})
	if err != nil {
		t.Fatalf("NewWorker() error: %v", err)
	}
	d.ptyBackend = workerBackend

	settings = d.settingsWithAgentAvailability()
	if got := settings[SettingPTYBackendMode]; got != "worker" {
		t.Fatalf("settings[%s] = %v, want worker", SettingPTYBackendMode, got)
	}
}

func TestDaemon_ApprovePR_ViaWebSocket(t *testing.T) {
	// Create mock GitHub server
	mockGH := mockserver.New()
	defer mockGH.Close()

	// Add a mock PR
	mockGH.AddPR(mockserver.MockPR{
		Repo:   "test/repo",
		Number: 42,
		Title:  "Test PR",
		Draft:  false,
		Role:   "reviewer",
	})

	// Use unique port to avoid conflicts
	wsPort := "19849"
	os.Setenv("ATTN_WS_PORT", wsPort)
	defer os.Unsetenv("ATTN_WS_PORT")

	// Create GitHub client pointing to mock server
	ghClient, err := github.NewClient(mockGH.URL, "test-token")
	if err != nil {
		t.Fatalf("NewClient error: %v", err)
	}
	t.Setenv("ATTN_MOCK_GH_URL", mockGH.URL)
	t.Setenv("ATTN_MOCK_GH_TOKEN", "test-token")
	t.Setenv("ATTN_MOCK_GH_HOST", ghClient.Host())

	// Create daemon with GitHub client
	// Use /tmp directly to avoid long socket paths, with unique suffix to prevent parallel test conflicts
	sockPath := filepath.Join("/tmp", fmt.Sprintf("attn-test-ws-%d.sock", time.Now().UnixNano()))
	os.Remove(sockPath) // Clean up any existing socket
	d := NewWithGitHubClient(sockPath, ghClient)

	// Start daemon in background
	go func() {
		if err := d.Start(); err != nil {
			t.Logf("Daemon start error: %v", err)
		}
	}()
	defer func() {
		d.Stop()
		os.Remove(sockPath)
	}()

	// Wait for daemon and WebSocket server to start (with retries)
	// First wait for the unix socket to be ready
	time.Sleep(200 * time.Millisecond)

	ctx := context.Background()
	wsURL := "ws://127.0.0.1:" + wsPort + "/ws"
	var conn *websocket.Conn
	maxRetries := 20
	for i := 0; i < maxRetries; i++ {
		time.Sleep(100 * time.Millisecond)
		var dialErr error
		conn, _, dialErr = websocket.Dial(ctx, wsURL, nil)
		if dialErr == nil {
			t.Logf("WebSocket connected successfully after %d retries", i+1)
			break
		}
		if i == maxRetries-1 {
			t.Fatalf("WebSocket dial error after %d retries: %v", maxRetries, dialErr)
		}
	}
	defer conn.Close(websocket.StatusNormalClosure, "")

	// Read initial state
	_, initialData, err := conn.Read(ctx)
	if err != nil {
		t.Fatalf("Read initial state error: %v", err)
	}
	t.Logf("Initial state: %s", string(initialData))

	// Send approve command
	prID := protocol.FormatPRID(ghClient.Host(), "test/repo", 42)
	approveCmd := map[string]interface{}{
		"cmd": "approve_pr",
		"id":  prID,
	}
	approveJSON, _ := json.Marshal(approveCmd)
	err = conn.Write(ctx, websocket.MessageText, approveJSON)
	if err != nil {
		t.Fatalf("Write approve command error: %v", err)
	}

	// Read responses until we get pr_action_result (prs_updated may come first due to background polling)
	var response protocol.PRActionResultMessage
	for i := 0; i < 10; i++ {
		_, responseData, err := conn.Read(ctx)
		if err != nil {
			t.Fatalf("Read response error: %v", err)
		}
		t.Logf("Response %d: %s", i+1, string(responseData))

		// Check if this is the pr_action_result event
		var eventCheck struct {
			Event string `json:"event"`
		}
		json.Unmarshal(responseData, &eventCheck)
		if eventCheck.Event == "pr_action_result" {
			err = json.Unmarshal(responseData, &response)
			if err != nil {
				t.Fatalf("Unmarshal response error: %v", err)
			}
			break
		}
		// Otherwise it's probably prs_updated from background polling, continue reading
	}

	// Verify response
	if !response.Success {
		t.Errorf("Expected success=true, got success=%v, error=%s", response.Success, protocol.Deref(response.Error))
	}
	if response.Action != "approve" {
		t.Errorf("Expected action=approve, got action=%s", response.Action)
	}
	if response.ID != prID {
		t.Errorf("Expected id=%s, got id=%s", prID, response.ID)
	}

	// Verify mock server received the approve request
	if !mockGH.HasApproveRequest("test/repo", 42) {
		t.Error("Mock server did not receive approve request for test/repo#42")
	}
}

func TestDaemon_InjectTestPR(t *testing.T) {
	t.Setenv("ATTN_WS_PORT", "19905")

	tmpDir := t.TempDir()
	sockPath := filepath.Join(tmpDir, "test.sock")

	d := NewForTesting(sockPath)
	go d.Start()
	defer d.Stop()

	waitForSocket(t, sockPath, 5*time.Second)

	c := client.New(sockPath)

	// Create test PR data
	testPR := protocol.PR{
		ID:          "github.com:owner/repo#123",
		Repo:        "owner/repo",
		Number:      123,
		Title:       "Test PR for E2E",
		URL:         "https://github.com/owner/repo/pull/123",
		Role:        protocol.PRRoleAuthor,
		State:       protocol.PRStateWaiting,
		Reason:      protocol.PRReasonReadyToMerge,
		LastUpdated: protocol.TimestampNow().String(),
		LastPolled:  protocol.TimestampNow().String(),
		Muted:       false,
	}

	// Send inject_test_pr message
	msg := protocol.InjectTestPRMessage{
		Cmd: protocol.CmdInjectTestPR,
		PR:  testPR,
	}
	msgJSON, _ := json.Marshal(msg)

	conn, err := net.Dial("unix", sockPath)
	if err != nil {
		t.Fatalf("Dial error: %v", err)
	}
	defer conn.Close()

	_, err = conn.Write(msgJSON)
	if err != nil {
		t.Fatalf("Write error: %v", err)
	}

	// Read response
	var resp protocol.Response
	err = json.NewDecoder(conn).Decode(&resp)
	if err != nil {
		t.Fatalf("Decode response error: %v", err)
	}

	if !resp.Ok {
		t.Fatalf("Expected Ok=true, got Ok=%v, Error=%s", resp.Ok, protocol.Deref(resp.Error))
	}

	// Verify PR was added using query_prs
	prs, err := c.QueryPRs("")
	if err != nil {
		t.Fatalf("QueryPRs error: %v", err)
	}

	if len(prs) != 1 {
		t.Fatalf("Expected 1 PR, got %d", len(prs))
	}

	if prs[0].ID != "github.com:owner/repo#123" {
		t.Errorf("Expected ID=github.com:owner/repo#123, got ID=%s", prs[0].ID)
	}
	if prs[0].Title != "Test PR for E2E" {
		t.Errorf("Expected Title='Test PR for E2E', got Title=%s", prs[0].Title)
	}
	if prs[0].State != protocol.PRStateWaiting {
		t.Errorf("Expected State=waiting, got State=%s", prs[0].State)
	}
}

func TestDaemon_MutePR_ViaWebSocket(t *testing.T) {
	// Use unique port to avoid conflicts
	wsPort := "19850"
	os.Setenv("ATTN_WS_PORT", wsPort)
	defer os.Unsetenv("ATTN_WS_PORT")

	// Use /tmp directly to avoid long socket paths, with unique suffix to prevent parallel test conflicts
	sockPath := filepath.Join("/tmp", fmt.Sprintf("attn-test-mute-pr-%d.sock", time.Now().UnixNano()))
	os.Remove(sockPath) // Clean up any existing socket

	d := NewForTesting(sockPath)

	// Start daemon in background
	go func() {
		if err := d.Start(); err != nil {
			t.Logf("Daemon start error: %v", err)
		}
	}()
	defer func() {
		d.Stop()
		os.Remove(sockPath)
	}()

	// Wait for daemon to start
	time.Sleep(200 * time.Millisecond)

	// Inject test PR via unix socket
	testPR := protocol.PR{
		ID:          "github.com:owner/repo#123",
		Repo:        "owner/repo",
		Number:      123,
		Title:       "Test PR",
		URL:         "https://github.com/owner/repo/pull/123",
		Role:        protocol.PRRoleAuthor,
		State:       protocol.PRStateWaiting,
		Reason:      protocol.PRReasonReadyToMerge,
		LastUpdated: protocol.TimestampNow().String(),
		LastPolled:  protocol.TimestampNow().String(),
		Muted:       false,
	}
	msg := protocol.InjectTestPRMessage{
		Cmd: protocol.CmdInjectTestPR,
		PR:  testPR,
	}
	msgJSON, _ := json.Marshal(msg)
	conn, err := net.Dial("unix", sockPath)
	if err != nil {
		t.Fatalf("Dial error: %v", err)
	}
	if _, err := conn.Write(msgJSON); err != nil {
		t.Fatalf("Write inject PR error: %v", err)
	}
	var resp protocol.Response
	if err := json.NewDecoder(conn).Decode(&resp); err != nil {
		t.Fatalf("Read inject PR response error: %v", err)
	}
	if !resp.Ok {
		t.Fatalf("Inject PR failed: %s", protocol.Deref(resp.Error))
	}
	conn.Close()

	// Connect to WebSocket
	ctx := context.Background()
	wsURL := "ws://127.0.0.1:" + wsPort + "/ws"
	var wsConn *websocket.Conn
	maxRetries := 20
	for i := 0; i < maxRetries; i++ {
		time.Sleep(100 * time.Millisecond)
		var dialErr error
		wsConn, _, dialErr = websocket.Dial(ctx, wsURL, nil)
		if dialErr == nil {
			break
		}
		if i == maxRetries-1 {
			t.Fatalf("WebSocket dial error after %d retries: %v", maxRetries, dialErr)
		}
	}
	defer wsConn.Close(websocket.StatusNormalClosure, "")

	// Read initial state
	_, initialData, err := wsConn.Read(ctx)
	if err != nil {
		t.Fatalf("Read initial state error: %v", err)
	}

	// Verify PR is not muted in initial state
	var initialState protocol.WebSocketEvent
	json.Unmarshal(initialData, &initialState)
	if len(initialState.Prs) != 1 {
		t.Fatalf("Expected 1 PR in initial state, got %d", len(initialState.Prs))
	}
	if initialState.Prs[0].Muted {
		t.Error("Expected PR to not be muted initially")
	}

	// Send mute_pr command
	muteCmd := map[string]interface{}{
		"cmd": "mute_pr",
		"id":  "github.com:owner/repo#123",
	}
	muteJSON, _ := json.Marshal(muteCmd)
	err = wsConn.Write(ctx, websocket.MessageText, muteJSON)
	if err != nil {
		t.Fatalf("Write mute command error: %v", err)
	}

	// Read prs_updated broadcast
	_, updateData, err := wsConn.Read(ctx)
	if err != nil {
		t.Fatalf("Read update error: %v", err)
	}

	var updateEvent protocol.WebSocketEvent
	json.Unmarshal(updateData, &updateEvent)
	if updateEvent.Event != protocol.EventPRsUpdated {
		t.Errorf("Expected event=%s, got event=%s", protocol.EventPRsUpdated, updateEvent.Event)
	}
	if len(updateEvent.Prs) != 1 {
		t.Fatalf("Expected 1 PR in update, got %d", len(updateEvent.Prs))
	}
	if !updateEvent.Prs[0].Muted {
		t.Error("Expected PR to be muted after mute command")
	}

	// Send mute_pr again to toggle back
	err = wsConn.Write(ctx, websocket.MessageText, muteJSON)
	if err != nil {
		t.Fatalf("Write second mute command error: %v", err)
	}

	// Read second prs_updated broadcast
	_, updateData2, err := wsConn.Read(ctx)
	if err != nil {
		t.Fatalf("Read second update error: %v", err)
	}

	var updateEvent2 protocol.WebSocketEvent
	json.Unmarshal(updateData2, &updateEvent2)
	if updateEvent2.Prs[0].Muted {
		t.Error("Expected PR to be unmuted after second mute command (toggle)")
	}
}

func TestDaemon_MuteRepo_ViaWebSocket(t *testing.T) {
	// Use unique port to avoid conflicts
	wsPort := "19851"
	os.Setenv("ATTN_WS_PORT", wsPort)
	defer os.Unsetenv("ATTN_WS_PORT")

	// Use /tmp directly to avoid long socket paths, with unique suffix to prevent parallel test conflicts
	sockPath := filepath.Join("/tmp", fmt.Sprintf("attn-test-mute-repo-%d.sock", time.Now().UnixNano()))
	os.Remove(sockPath) // Clean up any existing socket

	d := NewForTesting(sockPath)

	// Start daemon in background
	go func() {
		if err := d.Start(); err != nil {
			t.Logf("Daemon start error: %v", err)
		}
	}()
	defer func() {
		d.Stop()
		os.Remove(sockPath)
	}()

	// Wait for daemon to start
	time.Sleep(200 * time.Millisecond)

	// Connect to WebSocket
	ctx := context.Background()
	wsURL := "ws://127.0.0.1:" + wsPort + "/ws"
	var wsConn *websocket.Conn
	maxRetries := 20
	for i := 0; i < maxRetries; i++ {
		time.Sleep(100 * time.Millisecond)
		var dialErr error
		wsConn, _, dialErr = websocket.Dial(ctx, wsURL, nil)
		if dialErr == nil {
			break
		}
		if i == maxRetries-1 {
			t.Fatalf("WebSocket dial error after %d retries: %v", maxRetries, dialErr)
		}
	}
	defer wsConn.Close(websocket.StatusNormalClosure, "")

	// Read initial state
	_, initialData, err := wsConn.Read(ctx)
	if err != nil {
		t.Fatalf("Read initial state error: %v", err)
	}

	// Verify repos array exists in initial state (will be empty since no repos muted yet)
	var initialState protocol.WebSocketEvent
	json.Unmarshal(initialData, &initialState)
	// Note: Repos can be empty but should be present (may be nil if JSON doesn't include empty arrays)
	// This is fine - we just test that after muting, we get updates

	// Send mute_repo command
	muteCmd := map[string]interface{}{
		"cmd":  "mute_repo",
		"repo": "owner/test-repo",
	}
	muteJSON, _ := json.Marshal(muteCmd)
	err = wsConn.Write(ctx, websocket.MessageText, muteJSON)
	if err != nil {
		t.Fatalf("Write mute_repo command error: %v", err)
	}

	// Read repos_updated broadcast
	_, updateData, err := wsConn.Read(ctx)
	if err != nil {
		t.Fatalf("Read update error: %v", err)
	}

	var updateEvent protocol.WebSocketEvent
	json.Unmarshal(updateData, &updateEvent)
	if updateEvent.Event != protocol.EventReposUpdated {
		t.Errorf("Expected event=%s, got event=%s", protocol.EventReposUpdated, updateEvent.Event)
	}
	if len(updateEvent.Repos) != 1 {
		t.Fatalf("Expected 1 repo state in update, got %d", len(updateEvent.Repos))
	}
	if updateEvent.Repos[0].Repo != "owner/test-repo" {
		t.Errorf("Expected repo=owner/test-repo, got repo=%s", updateEvent.Repos[0].Repo)
	}
	if !updateEvent.Repos[0].Muted {
		t.Error("Expected repo to be muted after mute_repo command")
	}

	// Send mute_repo again to toggle back
	err = wsConn.Write(ctx, websocket.MessageText, muteJSON)
	if err != nil {
		t.Fatalf("Write second mute_repo command error: %v", err)
	}

	// Read second repos_updated broadcast
	_, updateData2, err := wsConn.Read(ctx)
	if err != nil {
		t.Fatalf("Read second update error: %v", err)
	}

	var updateEvent2 protocol.WebSocketEvent
	json.Unmarshal(updateData2, &updateEvent2)
	if updateEvent2.Repos[0].Muted {
		t.Error("Expected repo to be unmuted after second mute_repo command (toggle)")
	}
}

func TestDaemon_InitialState_IncludesRepoStates(t *testing.T) {
	// Use unique port to avoid conflicts
	wsPort := "19852"
	os.Setenv("ATTN_WS_PORT", wsPort)
	defer os.Unsetenv("ATTN_WS_PORT")

	// Use /tmp directly to avoid long socket paths, with unique suffix to prevent parallel test conflicts
	sockPath := filepath.Join("/tmp", fmt.Sprintf("attn-test-initial-repos-%d.sock", time.Now().UnixNano()))
	os.Remove(sockPath) // Clean up any existing socket

	d := NewForTesting(sockPath)

	// Start daemon in background
	go func() {
		if err := d.Start(); err != nil {
			t.Logf("Daemon start error: %v", err)
		}
	}()
	defer func() {
		d.Stop()
		os.Remove(sockPath)
	}()

	// Wait for daemon to start
	time.Sleep(200 * time.Millisecond)

	// First, toggle a repo mute via unix socket to set up state
	c := client.New(sockPath)
	err := c.ToggleMuteRepo("owner/test-repo")
	if err != nil {
		t.Fatalf("ToggleMuteRepo error: %v", err)
	}

	// Connect to WebSocket
	ctx := context.Background()
	wsURL := "ws://127.0.0.1:" + wsPort + "/ws"
	var wsConn *websocket.Conn
	maxRetries := 20
	for i := 0; i < maxRetries; i++ {
		time.Sleep(100 * time.Millisecond)
		var dialErr error
		wsConn, _, dialErr = websocket.Dial(ctx, wsURL, nil)
		if dialErr == nil {
			break
		}
		if i == maxRetries-1 {
			t.Fatalf("WebSocket dial error after %d retries: %v", maxRetries, dialErr)
		}
	}
	defer wsConn.Close(websocket.StatusNormalClosure, "")

	// Read initial state
	_, initialData, err := wsConn.Read(ctx)
	if err != nil {
		t.Fatalf("Read initial state error: %v", err)
	}

	var initialState protocol.WebSocketEvent
	json.Unmarshal(initialData, &initialState)

	// Verify initial state includes repos
	if initialState.Event != protocol.EventInitialState {
		t.Errorf("Expected event=%s, got event=%s", protocol.EventInitialState, initialState.Event)
	}
	if initialState.Repos == nil {
		t.Fatal("Expected Repos array in initial state")
	}
	if len(initialState.Repos) != 1 {
		t.Fatalf("Expected 1 repo in initial state, got %d", len(initialState.Repos))
	}
	if initialState.Repos[0].Repo != "owner/test-repo" {
		t.Errorf("Expected repo=owner/test-repo, got repo=%s", initialState.Repos[0].Repo)
	}
	if !initialState.Repos[0].Muted {
		t.Error("Expected repo to be muted in initial state")
	}
}

// ============================================================================
// Session State Flow Tests
// ============================================================================

func TestDaemon_StateChange_BroadcastsToWebSocket(t *testing.T) {
	// Use unique port to avoid conflicts
	wsPort := "19853"
	os.Setenv("ATTN_WS_PORT", wsPort)
	defer os.Unsetenv("ATTN_WS_PORT")

	sockPath := filepath.Join("/tmp", fmt.Sprintf("attn-test-state-broadcast-%d.sock", time.Now().UnixNano()))
	os.Remove(sockPath)

	d := NewForTesting(sockPath)
	go func() {
		if err := d.Start(); err != nil {
			t.Logf("Daemon start error: %v", err)
		}
	}()
	defer func() {
		d.Stop()
		os.Remove(sockPath)
	}()

	waitForSocket(t, sockPath, 5*time.Second)

	// Register session via unix socket
	c := client.New(sockPath)
	err := c.Register("test-session", "Test Session", "/tmp/test")
	if err != nil {
		t.Fatalf("Register error: %v", err)
	}

	// Connect to WebSocket
	ctx := context.Background()
	wsURL := "ws://127.0.0.1:" + wsPort + "/ws"
	var wsConn *websocket.Conn
	maxRetries := 20
	for i := 0; i < maxRetries; i++ {
		time.Sleep(100 * time.Millisecond)
		var dialErr error
		wsConn, _, dialErr = websocket.Dial(ctx, wsURL, nil)
		if dialErr == nil {
			break
		}
		if i == maxRetries-1 {
			t.Fatalf("WebSocket dial error after %d retries: %v", maxRetries, dialErr)
		}
	}
	defer wsConn.Close(websocket.StatusNormalClosure, "")

	// Read initial state
	_, _, err = wsConn.Read(ctx)
	if err != nil {
		t.Fatalf("Read initial state error: %v", err)
	}

	// Update state to waiting_input via unix socket
	err = c.UpdateState("test-session", protocol.StateWaitingInput)
	if err != nil {
		t.Fatalf("UpdateState error: %v", err)
	}

	// Read WebSocket event - should be session_state_changed
	_, eventData, err := wsConn.Read(ctx)
	if err != nil {
		t.Fatalf("Read event error: %v", err)
	}

	var event protocol.WebSocketEvent
	json.Unmarshal(eventData, &event)

	if event.Event != protocol.EventSessionStateChanged {
		t.Errorf("Expected event=%s, got event=%s", protocol.EventSessionStateChanged, event.Event)
	}
	if event.Session == nil {
		t.Fatal("Expected Session in event")
	}
	if event.Session.ID != "test-session" {
		t.Errorf("Expected session id=test-session, got id=%s", event.Session.ID)
	}
	if event.Session.State != protocol.SessionStateWaitingInput {
		t.Errorf("Expected state=%s, got state=%s", protocol.SessionStateWaitingInput, event.Session.State)
	}
}

func TestDaemon_StateTransitions_AllStates(t *testing.T) {
	// Use unique port to avoid conflicts
	wsPort := "19854"
	os.Setenv("ATTN_WS_PORT", wsPort)
	defer os.Unsetenv("ATTN_WS_PORT")

	sockPath := filepath.Join("/tmp", fmt.Sprintf("attn-test-state-transitions-%d.sock", time.Now().UnixNano()))
	os.Remove(sockPath)

	d := NewForTesting(sockPath)
	go func() {
		if err := d.Start(); err != nil {
			t.Logf("Daemon start error: %v", err)
		}
	}()
	defer func() {
		d.Stop()
		os.Remove(sockPath)
	}()

	waitForSocket(t, sockPath, 5*time.Second)

	c := client.New(sockPath)
	err := c.Register("test-session", "Test", "/tmp/test")
	if err != nil {
		t.Fatalf("Register error: %v", err)
	}

	// Connect to WebSocket
	ctx := context.Background()
	wsURL := "ws://127.0.0.1:" + wsPort + "/ws"
	var wsConn *websocket.Conn
	maxRetries := 20
	for i := 0; i < maxRetries; i++ {
		time.Sleep(100 * time.Millisecond)
		var dialErr error
		wsConn, _, dialErr = websocket.Dial(ctx, wsURL, nil)
		if dialErr == nil {
			break
		}
		if i == maxRetries-1 {
			t.Fatalf("WebSocket dial error after %d retries: %v", maxRetries, dialErr)
		}
	}
	defer wsConn.Close(websocket.StatusNormalClosure, "")

	// Read initial state
	_, _, err = wsConn.Read(ctx)
	if err != nil {
		t.Fatalf("Read initial state error: %v", err)
	}

	// Test state transitions after register default (launching)
	states := []string{protocol.StateWaitingInput, protocol.StateIdle, protocol.StateWorking, protocol.StateUnknown}

	for _, expectedState := range states {
		err = c.UpdateState("test-session", expectedState)
		if err != nil {
			t.Fatalf("UpdateState to %s error: %v", expectedState, err)
		}

		// Read and verify event
		_, eventData, err := wsConn.Read(ctx)
		if err != nil {
			t.Fatalf("Read event error for state %s: %v", expectedState, err)
		}

		var event protocol.WebSocketEvent
		json.Unmarshal(eventData, &event)

		if event.Event != protocol.EventSessionStateChanged {
			t.Errorf("Expected event=%s for state %s, got event=%s", protocol.EventSessionStateChanged, expectedState, event.Event)
		}
		// Compare state - need to handle string/SessionState conversion
		if string(event.Session.State) != expectedState {
			t.Errorf("Expected state=%s, got state=%s", expectedState, event.Session.State)
		}
	}
}

func TestDaemon_InjectTestSession_BroadcastsToWebSocket(t *testing.T) {
	// Use unique port to avoid conflicts
	wsPort := "19855"
	os.Setenv("ATTN_WS_PORT", wsPort)
	defer os.Unsetenv("ATTN_WS_PORT")

	sockPath := filepath.Join("/tmp", fmt.Sprintf("attn-test-inject-session-%d.sock", time.Now().UnixNano()))
	os.Remove(sockPath)

	d := NewForTesting(sockPath)
	go func() {
		if err := d.Start(); err != nil {
			t.Logf("Daemon start error: %v", err)
		}
	}()
	defer func() {
		d.Stop()
		os.Remove(sockPath)
	}()

	waitForSocket(t, sockPath, 5*time.Second)

	// Connect to WebSocket first
	ctx := context.Background()
	wsURL := "ws://127.0.0.1:" + wsPort + "/ws"
	var wsConn *websocket.Conn
	maxRetries := 20
	for i := 0; i < maxRetries; i++ {
		time.Sleep(100 * time.Millisecond)
		var dialErr error
		wsConn, _, dialErr = websocket.Dial(ctx, wsURL, nil)
		if dialErr == nil {
			break
		}
		if i == maxRetries-1 {
			t.Fatalf("WebSocket dial error after %d retries: %v", maxRetries, dialErr)
		}
	}
	defer wsConn.Close(websocket.StatusNormalClosure, "")

	// Read initial state
	_, _, err := wsConn.Read(ctx)
	if err != nil {
		t.Fatalf("Read initial state error: %v", err)
	}

	// Inject test session via unix socket
	conn, err := net.Dial("unix", sockPath)
	if err != nil {
		t.Fatalf("Dial error: %v", err)
	}

	injectMsg := map[string]interface{}{
		"cmd": "inject_test_session",
		"session": map[string]interface{}{
			"id":          "injected-session",
			"label":       "Injected Session",
			"directory":   "/tmp/injected",
			"state":       protocol.StateWorking,
			"state_since": time.Now().Format(time.RFC3339),
			"last_seen":   time.Now().Format(time.RFC3339),
			"muted":       false,
		},
	}
	msgJSON, _ := json.Marshal(injectMsg)
	conn.Write(msgJSON)
	conn.Close()

	// Read WebSocket event - should be session_registered
	_, eventData, err := wsConn.Read(ctx)
	if err != nil {
		t.Fatalf("Read event error: %v", err)
	}

	var event protocol.WebSocketEvent
	json.Unmarshal(eventData, &event)

	if event.Event != protocol.EventSessionRegistered {
		t.Errorf("Expected event=%s, got event=%s", protocol.EventSessionRegistered, event.Event)
	}
	if event.Session == nil {
		t.Fatal("Expected Session in event")
	}
	if event.Session.ID != "injected-session" {
		t.Errorf("Expected session id=injected-session, got id=%s", event.Session.ID)
	}
	if event.Session.State != protocol.SessionStateWorking {
		t.Errorf("Expected state=%s, got state=%s", protocol.SessionStateWorking, event.Session.State)
	}
}

func TestDaemon_StopCommand_PendingTodos_SetsWaitingInput(t *testing.T) {
	t.Setenv("ATTN_WS_PORT", "19906")

	// Use /tmp directly to avoid long socket paths
	sockPath := filepath.Join("/tmp", fmt.Sprintf("attn-test-stop-pending-%d.sock", time.Now().UnixNano()))
	os.Remove(sockPath) // Clean up any existing socket

	d := NewForTesting(sockPath)
	go d.Start()
	defer func() {
		d.Stop()
		os.Remove(sockPath)
	}()

	time.Sleep(100 * time.Millisecond)

	c := client.New(sockPath)

	// Register session
	err := c.Register("test-session", "Test", "/tmp/test")
	if err != nil {
		t.Fatalf("Register error: %v", err)
	}

	// Send todos with pending items
	conn, err := net.Dial("unix", sockPath)
	if err != nil {
		t.Fatalf("Dial error: %v", err)
	}
	todosMsg := map[string]interface{}{
		"cmd":   "todos",
		"id":    "test-session",
		"todos": []string{"[ ] Pending task 1", "[ ] Pending task 2"},
	}
	todosJSON, _ := json.Marshal(todosMsg)
	conn.Write(todosJSON)

	// Read response
	var resp protocol.Response
	json.NewDecoder(conn).Decode(&resp)
	conn.Close()

	if !resp.Ok {
		t.Fatalf("Todos update failed: %s", protocol.Deref(resp.Error))
	}

	// Send stop command (should classify as waiting_input due to pending todos)
	conn2, err := net.Dial("unix", sockPath)
	if err != nil {
		t.Fatalf("Dial error: %v", err)
	}
	stopMsg := map[string]interface{}{
		"cmd":             "stop",
		"id":              "test-session",
		"transcript_path": "/nonexistent/path", // Doesn't matter - pending todos short-circuit
	}
	stopJSON, _ := json.Marshal(stopMsg)
	conn2.Write(stopJSON)
	json.NewDecoder(conn2).Decode(&resp)
	conn2.Close()

	// Wait for async classification to complete
	time.Sleep(200 * time.Millisecond)

	// Query session state
	sessions, err := c.Query("")
	if err != nil {
		t.Fatalf("Query error: %v", err)
	}
	if len(sessions) != 1 {
		t.Fatalf("Expected 1 session, got %d", len(sessions))
	}
	if sessions[0].State != protocol.SessionStateWaitingInput {
		t.Errorf("Expected state=%s (due to pending todos), got state=%s", protocol.SessionStateWaitingInput, sessions[0].State)
	}
}

func TestDaemon_StopCommand_CompletedTodos_ProceedsToClassification(t *testing.T) {
	t.Setenv("ATTN_WS_PORT", "19907")

	// This test verifies that when all todos are completed, the daemon
	// does NOT short-circuit to waiting_input based on todos alone.
	// Instead, it proceeds to classification.
	//
	// When transcript parsing fails, it now returns unknown,
	// but that's different from the todos short-circuit path.

	// Use /tmp directly to avoid long socket paths
	sockPath := filepath.Join("/tmp", fmt.Sprintf("attn-test-stop-completed-%d.sock", time.Now().UnixNano()))
	os.Remove(sockPath) // Clean up any existing socket

	d := NewForTesting(sockPath)
	go d.Start()
	defer func() {
		d.Stop()
		os.Remove(sockPath)
	}()

	waitForSocket(t, sockPath, 2*time.Second)

	c := client.New(sockPath)

	// Register session
	err := c.Register("test-session", "Test", "/tmp/test")
	if err != nil {
		t.Fatalf("Register error: %v", err)
	}

	// Send todos with ALL completed items (using [] prefix)
	conn, err := net.Dial("unix", sockPath)
	if err != nil {
		t.Fatalf("Dial error: %v", err)
	}
	todosMsg := map[string]interface{}{
		"cmd":   "todos",
		"id":    "test-session",
		"todos": []string{"[] Completed task 1", "[] Completed task 2"},
	}
	todosJSON, _ := json.Marshal(todosMsg)
	conn.Write(todosJSON)

	var resp protocol.Response
	json.NewDecoder(conn).Decode(&resp)
	conn.Close()

	if !resp.Ok {
		t.Fatalf("Todos update failed: %s", protocol.Deref(resp.Error))
	}

	// Verify todos were stored correctly
	sessions, _ := c.Query("")
	if len(sessions) != 1 {
		t.Fatalf("Expected 1 session, got %d", len(sessions))
	}
	if len(sessions[0].Todos) != 2 {
		t.Fatalf("Expected 2 todos, got %d", len(sessions[0].Todos))
	}

	// With all completed todos, stop should proceed to classification (not short-circuit)
	// Since we're providing a nonexistent transcript, classification will fail
	// and return unknown - but this is different from todos short-circuit
	//
	// The key difference:
	// - With pending todos: immediately returns waiting_input (no transcript parsing)
	// - With completed todos: tries to parse transcript, then classify
	//
	// This test mainly ensures the todos count logic correctly skips completed todos
	t.Log("Test passed: todos with [] prefix are counted as completed, allowing classification to proceed")
}

func TestClassifySessionState_ClassifierError_StaysUnknown(t *testing.T) {
	d := NewForTesting(filepath.Join(t.TempDir(), "test.sock"))
	d.classifier = &errorClassifier{
		state: protocol.StateUnknown,
		err:   errors.New("classifier execution failed"),
	}

	now := time.Now()
	nowStr := string(protocol.NewTimestamp(now))
	d.store.Add(&protocol.Session{
		ID:             "sess-unknown",
		Agent:          protocol.SessionAgentCodex,
		Label:          "test",
		Directory:      "/tmp",
		State:          protocol.StateWorking,
		StateSince:     nowStr,
		StateUpdatedAt: nowStr,
		LastSeen:       nowStr,
	})

	transcriptPath := filepath.Join(t.TempDir(), "transcript.jsonl")
	content := `{"type":"assistant","message":{"role":"assistant","content":"Now running pre-review."}}
`
	if err := os.WriteFile(transcriptPath, []byte(content), 0644); err != nil {
		t.Fatalf("write transcript: %v", err)
	}

	d.classifySessionState("sess-unknown", transcriptPath)

	sess := d.store.Get("sess-unknown")
	if sess == nil {
		t.Fatal("session missing after classify")
	}
	if sess.State != protocol.StateUnknown {
		t.Fatalf("state = %s, want %s", sess.State, protocol.StateUnknown)
	}
}

func TestClassifySessionState_ClaudeSkipsDuplicateAssistantTurn(t *testing.T) {
	d := NewForTesting(filepath.Join(t.TempDir(), "test.sock"))
	mockClassifier := &countingClassifier{state: protocol.StateWaitingInput}
	d.classifier = mockClassifier

	now := time.Now()
	nowStr := string(protocol.NewTimestamp(now))
	d.store.Add(&protocol.Session{
		ID:             "sess-1",
		Agent:          protocol.SessionAgentClaude,
		Label:          "test",
		Directory:      "/tmp",
		State:          protocol.StateWorking,
		StateSince:     nowStr,
		StateUpdatedAt: nowStr,
		LastSeen:       nowStr,
	})

	transcriptPath := filepath.Join(t.TempDir(), "transcript.jsonl")
	content := fmt.Sprintf(
		`{"type":"user","uuid":"u1","timestamp":"%s","message":{"role":"user","content":"hello"}}
{"type":"assistant","uuid":"a1","timestamp":"%s","message":{"role":"assistant","content":[{"type":"text","text":"Hello! What can I help you with today?"}]}}
`,
		now.Add(-1*time.Second).UTC().Format(time.RFC3339Nano),
		now.UTC().Format(time.RFC3339Nano),
	)
	if err := os.WriteFile(transcriptPath, []byte(content), 0644); err != nil {
		t.Fatalf("write transcript: %v", err)
	}

	d.classifySessionState("sess-1", transcriptPath)
	if got := mockClassifier.CallCount(); got != 1 {
		t.Fatalf("first classify calls=%d, want 1", got)
	}

	sess := d.store.Get("sess-1")
	if sess == nil {
		t.Fatal("session missing after first classify")
	}
	firstState := sess.State

	d.classifySessionState("sess-1", transcriptPath)
	if got := mockClassifier.CallCount(); got != 1 {
		t.Fatalf("second classify calls=%d, want still 1", got)
	}

	sess = d.store.Get("sess-1")
	if sess == nil {
		t.Fatal("session missing after second classify")
	}
	if sess.State != firstState {
		t.Fatalf("state changed on duplicate turn: got %q want %q", sess.State, firstState)
	}
}

func TestClassifySessionState_ClaudeConcurrentDuplicateTurnRunsOnce(t *testing.T) {
	d := NewForTesting(filepath.Join(t.TempDir(), "test.sock"))
	mockClassifier := newBlockingClassifier(protocol.StateWaitingInput)
	d.classifier = mockClassifier

	now := time.Now()
	nowStr := string(protocol.NewTimestamp(now))
	d.store.Add(&protocol.Session{
		ID:             "sess-2",
		Agent:          protocol.SessionAgentClaude,
		Label:          "test",
		Directory:      "/tmp",
		State:          protocol.StateWorking,
		StateSince:     nowStr,
		StateUpdatedAt: nowStr,
		LastSeen:       nowStr,
	})

	transcriptPath := filepath.Join(t.TempDir(), "transcript.jsonl")
	content := fmt.Sprintf(
		`{"type":"user","uuid":"u2","timestamp":"%s","message":{"role":"user","content":"hello"}}
{"type":"assistant","uuid":"a2","timestamp":"%s","message":{"role":"assistant","content":[{"type":"text","text":"Hello! What can I help you with today?"}]}}
`,
		now.Add(-1*time.Second).UTC().Format(time.RFC3339Nano),
		now.UTC().Format(time.RFC3339Nano),
	)
	if err := os.WriteFile(transcriptPath, []byte(content), 0644); err != nil {
		t.Fatalf("write transcript: %v", err)
	}

	firstDone := make(chan struct{})
	go func() {
		d.classifySessionState("sess-2", transcriptPath)
		close(firstDone)
	}()

	select {
	case <-mockClassifier.started:
	case <-time.After(2 * time.Second):
		t.Fatal("classifier did not start for first classification")
	}

	secondDone := make(chan struct{})
	go func() {
		d.classifySessionState("sess-2", transcriptPath)
		close(secondDone)
	}()

	select {
	case <-secondDone:
	case <-time.After(2 * time.Second):
		t.Fatal("second classification did not return promptly")
	}

	if got := mockClassifier.CallCount(); got != 1 {
		t.Fatalf("classifier calls=%d, want 1 while duplicate turn in flight", got)
	}

	close(mockClassifier.release)
	select {
	case <-firstDone:
	case <-time.After(2 * time.Second):
		t.Fatal("first classification did not complete")
	}

	if got := mockClassifier.CallCount(); got != 1 {
		t.Fatalf("classifier calls=%d, want 1", got)
	}
}

func TestUpdateAndBroadcastStateWithTimestamp_StaleIdleDoesNotClearLongRunTracking(t *testing.T) {
	d := NewForTesting(filepath.Join(t.TempDir(), "test.sock"))

	now := time.Now()
	nowStr := string(protocol.NewTimestamp(now))
	d.store.Add(&protocol.Session{
		ID:             "sess-stale",
		Agent:          protocol.SessionAgentCodex,
		Label:          "stale",
		Directory:      "/tmp",
		State:          protocol.StateWaitingInput,
		StateSince:     nowStr,
		StateUpdatedAt: nowStr,
		LastSeen:       nowStr,
	})
	d.longRun["sess-stale"] = longRunSession{
		workingSince:       now.Add(-6 * time.Minute),
		deferredTranscript: "/tmp/transcript.jsonl",
		needsReview:        true,
	}

	d.updateAndBroadcastStateWithTimestamp("sess-stale", protocol.StateIdle, now.Add(-1*time.Minute))

	session := d.store.Get("sess-stale")
	if session == nil {
		t.Fatal("session missing")
	}
	if session.State != protocol.StateWaitingInput {
		t.Fatalf("state=%s, want %s", session.State, protocol.StateWaitingInput)
	}
	if !d.sessionNeedsReviewAfterLongRun("sess-stale") {
		t.Fatal("needs_review_after_long_run should remain set for stale timestamped update")
	}
}

func TestClassifyOrDeferAfterStop_LongRunDefersUntilVisualized(t *testing.T) {
	d := NewForTesting(filepath.Join(t.TempDir(), "test.sock"))
	mockClassifier := &countingClassifier{state: protocol.StateWaitingInput}
	d.classifier = mockClassifier

	now := time.Now()
	nowStr := string(protocol.NewTimestamp(now))
	d.store.Add(&protocol.Session{
		ID:             "sess-long",
		Agent:          protocol.SessionAgentCodex,
		Label:          "long",
		Directory:      "/tmp",
		State:          protocol.StateWorking,
		StateSince:     nowStr,
		StateUpdatedAt: nowStr,
		LastSeen:       nowStr,
	})
	d.longRun["sess-long"] = longRunSession{workingSince: now.Add(-6 * time.Minute)}

	transcriptPath := filepath.Join(t.TempDir(), "long-transcript.jsonl")
	content := `{"type":"assistant","message":{"role":"assistant","content":"Completed long run"}}` + "\n"
	if err := os.WriteFile(transcriptPath, []byte(content), 0644); err != nil {
		t.Fatalf("write transcript: %v", err)
	}

	d.classifyOrDeferAfterStop("sess-long", transcriptPath)

	if got := mockClassifier.CallCount(); got != 0 {
		t.Fatalf("classifier calls=%d, want 0 while long-run review is deferred", got)
	}
	if !d.sessionNeedsReviewAfterLongRun("sess-long") {
		t.Fatal("needs_review_after_long_run should be set for deferred long run")
	}

	session := d.store.Get("sess-long")
	if session == nil {
		t.Fatal("session missing")
	}
	if session.State != protocol.StateWaitingInput {
		t.Fatalf("state=%s, want %s", session.State, protocol.StateWaitingInput)
	}
	decorated := d.sessionForBroadcast(session)
	if decorated == nil || !protocol.Deref(decorated.NeedsReviewAfterLongRun) {
		t.Fatal("broadcast session should include needs_review_after_long_run=true")
	}

	d.handleSessionVisualized("sess-long")

	deadline := time.Now().Add(2 * time.Second)
	for mockClassifier.CallCount() == 0 && time.Now().Before(deadline) {
		time.Sleep(10 * time.Millisecond)
	}
	if got := mockClassifier.CallCount(); got != 1 {
		t.Fatalf("classifier calls=%d, want 1 after visualization", got)
	}
	if d.sessionNeedsReviewAfterLongRun("sess-long") {
		t.Fatal("needs_review_after_long_run should clear after visualization")
	}
}

func TestClassifyOrDeferAfterStop_LongRunKeepsPendingApprovalState(t *testing.T) {
	d := NewForTesting(filepath.Join(t.TempDir(), "test.sock"))
	mockClassifier := &countingClassifier{state: protocol.StateIdle}
	d.classifier = mockClassifier

	now := time.Now()
	nowStr := string(protocol.NewTimestamp(now))
	d.store.Add(&protocol.Session{
		ID:             "sess-pending",
		Agent:          protocol.SessionAgentCodex,
		Label:          "pending",
		Directory:      "/tmp",
		State:          protocol.StatePendingApproval,
		StateSince:     nowStr,
		StateUpdatedAt: nowStr,
		LastSeen:       nowStr,
	})
	d.longRun["sess-pending"] = longRunSession{workingSince: now.Add(-7 * time.Minute)}

	d.classifyOrDeferAfterStop("sess-pending", "")

	if got := mockClassifier.CallCount(); got != 0 {
		t.Fatalf("classifier calls=%d, want 0 while deferred", got)
	}
	if !d.sessionNeedsReviewAfterLongRun("sess-pending") {
		t.Fatal("needs_review_after_long_run should be set")
	}

	session := d.store.Get("sess-pending")
	if session == nil {
		t.Fatal("session missing")
	}
	if session.State != protocol.StatePendingApproval {
		t.Fatalf("state=%s, want %s", session.State, protocol.StatePendingApproval)
	}
}

func TestClassifyOrDeferAfterStop_ShortRunClassifiesImmediately(t *testing.T) {
	d := NewForTesting(filepath.Join(t.TempDir(), "test.sock"))
	mockClassifier := &countingClassifier{state: protocol.StateIdle}
	d.classifier = mockClassifier

	now := time.Now()
	nowStr := string(protocol.NewTimestamp(now))
	d.store.Add(&protocol.Session{
		ID:             "sess-short",
		Agent:          protocol.SessionAgentCodex,
		Label:          "short",
		Directory:      "/tmp",
		State:          protocol.StateWorking,
		StateSince:     nowStr,
		StateUpdatedAt: nowStr,
		LastSeen:       nowStr,
	})
	d.longRun["sess-short"] = longRunSession{workingSince: now.Add(-2 * time.Minute)}

	transcriptPath := filepath.Join(t.TempDir(), "short-transcript.jsonl")
	content := `{"type":"assistant","message":{"role":"assistant","content":"Quick done"}}` + "\n"
	if err := os.WriteFile(transcriptPath, []byte(content), 0644); err != nil {
		t.Fatalf("write transcript: %v", err)
	}

	d.classifyOrDeferAfterStop("sess-short", transcriptPath)

	if got := mockClassifier.CallCount(); got != 1 {
		t.Fatalf("classifier calls=%d, want 1 for short run", got)
	}
	if d.sessionNeedsReviewAfterLongRun("sess-short") {
		t.Fatal("needs_review_after_long_run should be false for short run")
	}

	session := d.store.Get("sess-short")
	if session == nil {
		t.Fatal("session missing")
	}
	if session.State != protocol.StateIdle {
		t.Fatalf("state=%s, want %s", session.State, protocol.StateIdle)
	}
}
